{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dacon_satellite_baseline",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/curiosity806/2020_dacon_satellite_precipitation/blob/Theo/Theos_Dacon_satellite_baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oi-MWY4HOblX",
        "colab_type": "code",
        "outputId": "ab3366bc-9bd3-4e6e-a5df-89f8abfb6911",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6La-zpczGzY",
        "colab_type": "code",
        "outputId": "62b8729e-f1d5-483c-c95d-ac54c33b139f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "from keras.layers import LeakyReLU\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import random\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Dropout, Conv2DTranspose, MaxPooling2D, BatchNormalization, Activation, concatenate, Input, GlobalAveragePooling2D\n",
        "from tensorflow.keras import Model\n",
        "import warnings\n",
        "import gc\n",
        " \n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YFPNtasGtc-",
        "colab_type": "text"
      },
      "source": [
        "## 1. Data collection and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJ9oJs3QykgG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1.1 Data Collection\n",
        "# train\n",
        "train_origin = np.load('/content/drive/My Drive/2020 Kaggle Study/data/yeonung/train.npy')[:,:,:,:12]\n",
        "\n",
        "# feature\n",
        "train_temp = train_origin[:,:,:,:9]\n",
        "\n",
        "train_landtype = train_origin[:,:,:,9]\n",
        "train_lon_GMI = train_origin[:,:,:,10]\n",
        "train_lat_GMI = train_origin[:,:,:,11]\n",
        "train_DEM = np.load(\"/content/drive/My Drive/2020 Kaggle Study/data/DEM_DPR/dem_train_DPR.npy\")\n",
        "\n",
        "# target\n",
        "train_rain = np.load(\"/content/drive/My Drive/2020 Kaggle Study/data/gmi_preci_32bits.npy\")\n",
        "\n",
        "# test\n",
        "test_origin = np.load('/content/drive/My Drive/2020 Kaggle Study/data/yeonung/test.npy')[:,:,:,:12]\n",
        "\n",
        "# feature\n",
        "test_temp = test_origin[:,:,:,:9]\n",
        "\n",
        "test_landtype = test_origin[:,:,:,9]\n",
        "test_lon_GMI = test_origin[:,:,:,10]\n",
        "test_lat_GMI = test_origin[:,:,:,11]\n",
        "test_DEM = np.load(\"/content/drive/My Drive/2020 Kaggle Study/data/DEM_DPR/dem_test_DPR.npy\")\n",
        "\n",
        "del train_origin\n",
        "\n",
        "all_temp = np.concatenate((train_temp, test_temp), axis = 0)\n",
        "del train_temp, test_temp\n",
        "all_landtype = np.concatenate((train_landtype, test_landtype), axis = 0)\n",
        "del train_landtype, test_landtype\n",
        "all_lon_GMI = np.concatenate((train_lon_GMI, test_lon_GMI), axis = 0)\n",
        "del train_lon_GMI, test_lon_GMI\n",
        "all_lat_GMI = np.concatenate((train_lat_GMI, test_lat_GMI), axis = 0)\n",
        "del train_lat_GMI, test_lat_GMI\n",
        "all_DEM = np.concatenate((train_DEM, test_DEM), axis = 0)\n",
        "del train_DEM, test_DEM"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zB4-ji5bKFdd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "10604eff-5024-48a3-c82d-d7f42f1ab8d7"
      },
      "source": [
        "all_temp.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(78761, 40, 40, 9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjwXLazCEid1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1.2 Data preprocessing\n",
        "def minmaxscaler(data_info, n):\n",
        "  from sklearn.preprocessing import MinMaxScaler\n",
        "  scaler = MinMaxScaler()\n",
        "  data_in = data_info.reshape(-1, 1)\n",
        "  scaler.fit(data_in)\n",
        "  data_in = scaler.transform(data_in)\n",
        "  data_in = data_in.reshape(-1)\n",
        "  data_info = data_in.reshape(n, 40, 40, 1)\n",
        "  return data_info\n",
        "\n",
        "def standardscaler(data_info):\n",
        "  from sklearn.preprocessing import StandardScaler\n",
        "  scaler = StandardScaler()\n",
        "  data_in = data_info.reshape(-1, 1)\n",
        "  scaler.fit(data_in)\n",
        "  data_in = scaler.transform(data_in)\n",
        "  data_in = data_in.reshape(-1)\n",
        "  data_info = data_in.reshape(78761, 40, 40, 1)\n",
        "  return data_info"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olbssEV6_AhM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "d8b0e749-fe96-477c-9e9e-59051b1b30dd"
      },
      "source": [
        "# temp - standard , land - categorization, dem - minmax, rain - minmax\n",
        "print(\"temp\")\n",
        "for i in range(0,9):\n",
        "  t_temp = standardscaler(all_temp[:,:,:,i])\n",
        "  print(\"cat : %d\" %i)\n",
        "  all_temp[:,:,:,i:i+1] = t_temp\n",
        "\n",
        "print(\"land\")\n",
        "t_land = all_landtype.reshape(-1)\n",
        "for i in range(len(t_land)):\n",
        "  t_land[i] = int(str(t_land[i])[0])\n",
        "all_landtype = t_land.reshape(78761,40, 40, 1)\n",
        "\n",
        "print(\"DEM\")\n",
        "all_DEM = minmaxscaler(all_DEM, 78761)\n",
        "\n",
        "print(\"GMI lon, lat\")\n",
        "all_lon_GMI = standardscaler(all_lon_GMI)\n",
        "all_lat_GMI = standardscaler(all_lat_GMI)\n",
        "\n",
        "print(\"train rain\")\n",
        "train_rain = np.where(train_rain>0, train_rain, 0)\n",
        "train_rain = minmaxscaler(train_rain, 76345)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "temp\n",
            "cat : 0\n",
            "cat : 1\n",
            "cat : 2\n",
            "cat : 3\n",
            "cat : 4\n",
            "cat : 5\n",
            "cat : 6\n",
            "cat : 7\n",
            "cat : 8\n",
            "land\n",
            "DEM\n",
            "GMI lon, lat\n",
            "train rain\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8Nm2VFrl5pu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = np.concatenate((all_temp[:76345,:,:,:], all_landtype[:76345,:,:,:], all_DEM[:76345,:,:,:], all_lon_GMI[:76345,:,:,:], all_lat_GMI[:76345,:,:,:], train_rain), axis = 3)\n",
        "test = np.concatenate((all_temp[76345:,:,:,:], all_landtype[76345:,:,:,:], all_DEM[76345:,:,:,:], all_lon_GMI[76345:,:,:,:], all_lat_GMI[76345:,:,:,:]), axis = 3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcXAzdyTNRZl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del all_temp, all_landtype, all_DEM, all_lon_GMI, all_lat_GMI, train_rain"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcaeY00rMd4a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = tf.convert_to_tensor(train, dtype=np.float32)\n",
        "train = train.numpy()\n",
        "test = tf.convert_to_tensor(test, dtype=np.float32)\n",
        "test = test.numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnOFP8UOQ9Ip",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "0dc90a3e-cb5c-43cb-f75c-b415df21c9d0"
      },
      "source": [
        "print(train.shape)\n",
        "print(test.shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(76345, 40, 40, 14)\n",
            "(2416, 40, 40, 13)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjkU3PtCNYy8",
        "colab_type": "text"
      },
      "source": [
        "## 2. 모델 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKjP-Uobze5P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 재생산성을 위해 시드 고정\n",
        "np.random.seed(7)\n",
        "random.seed(7)\n",
        "tf.random.set_seed(7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0wWXJKu09ld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(input_layer_c, input_layer_d, start_neurons):\n",
        "\n",
        "    # 40 x 40 -> 20 x 20\n",
        "    convc1 = Conv2D(start_neurons * 1, (3, 3), activation=tf.keras.layers.LeakyReLU(alpha=0.1), padding=\"same\", kernel_initializer='glorot_uniform')(input_layer_c)\n",
        "    convc1 = Conv2D(start_neurons * 1, (3, 3), activation=tf.keras.layers.LeakyReLU(alpha=0.1), padding=\"same\", kernel_initializer='glorot_uniform')(convc1)\n",
        "    poolc1 = BatchNormalization()(convc1)\n",
        "    poolc1 = MaxPooling2D((2, 2))(poolc1)\n",
        "    poolc1 = Dropout(0.25)(poolc1)\n",
        "    # 20 x 20 -> 10 x 10\n",
        "    convc2 = Conv2D(start_neurons * 2, (3, 3), activation=tf.keras.layers.LeakyReLU(alpha=0.1), padding=\"same\", kernel_initializer='glorot_uniform')(poolc1)\n",
        "    convc2 = Conv2D(start_neurons * 2, (3, 3), activation=tf.keras.layers.LeakyReLU(alpha=0.1), padding=\"same\", kernel_initializer='glorot_uniform')(convc2)\n",
        "    poolc2 = BatchNormalization()(convc2)\n",
        "    poolc2 = MaxPooling2D((2, 2))(poolc2)\n",
        "    poolc2 = Dropout(0.25)(poolc2)\n",
        "    # 10 x 10 \n",
        "    convmc = Conv2D(start_neurons * 4, (3, 3), activation=tf.keras.layers.LeakyReLU(alpha=0.1), padding=\"same\", kernel_initializer='glorot_uniform')(poolc2)\n",
        "    convmc = BatchNormalization()(convmc)\n",
        "\n",
        "    # 40 x 40 -> 20 x 20\n",
        "    convd1 = Conv2D(start_neurons * 1, (3, 3), activation=tf.keras.layers.LeakyReLU(alpha=0.1), padding=\"same\", kernel_initializer='glorot_uniform')(input_layer_d)\n",
        "    convd1 = Conv2D(start_neurons * 1, (3, 3), activation=tf.keras.layers.LeakyReLU(alpha=0.1), padding=\"same\", kernel_initializer='glorot_uniform')(convd1)\n",
        "    poold1 = BatchNormalization()(convd1)\n",
        "    poold1 = MaxPooling2D((2, 2))(poold1)\n",
        "    poold1 = Dropout(0.25)(poold1)\n",
        "    # 20 x 20 -> 10 x 10\n",
        "    convd2 = Conv2D(start_neurons * 2, (3, 3), activation=tf.keras.layers.LeakyReLU(alpha=0.1), padding=\"same\", kernel_initializer='glorot_uniform')(poold1)\n",
        "    convd2 = Conv2D(start_neurons * 2, (3, 3), activation=tf.keras.layers.LeakyReLU(alpha=0.1), padding=\"same\", kernel_initializer='glorot_uniform')(convd2)\n",
        "    poold2 = BatchNormalization()(convd2)\n",
        "    poold2 = MaxPooling2D((2, 2))(poold2)\n",
        "    poold2 = Dropout(0.25)(poold2)\n",
        "    # 10 x 10 \n",
        "    convmd = Conv2D(start_neurons * 4, (3, 3), activation=tf.keras.layers.LeakyReLU(alpha=0.1), padding=\"same\", kernel_initializer='glorot_uniform')(poold2)\n",
        "    convmd = BatchNormalization()(convmd)\n",
        "\n",
        "    convm = concatenate([convmc, convmd], axis = 3)\n",
        "    convm = BatchNormalization()(convm)\n",
        "\n",
        "    # 10 x 10 -> 20 x 20\n",
        "    deconv2 = Conv2DTranspose(start_neurons * 2, (3, 3), strides=(2, 2), activation=tf.keras.layers.LeakyReLU(alpha=0.1), padding=\"same\", kernel_initializer='glorot_uniform')(convm)\n",
        "    uconv2 = concatenate([deconv2, convc2, convd2])\n",
        "    uconv2 = Dropout(0.25)(uconv2)\n",
        "    uconv2 = Conv2D(start_neurons * 2, (3, 3), activation=tf.keras.layers.LeakyReLU(alpha=0.1), padding=\"same\", kernel_initializer='glorot_uniform')(uconv2)\n",
        "    uconv2 = Conv2D(start_neurons * 2, (3, 3), activation=tf.keras.layers.LeakyReLU(alpha=0.1), padding=\"same\", kernel_initializer='glorot_uniform')(uconv2)\n",
        "    uconv2 = BatchNormalization()(uconv2)\n",
        "\n",
        "    # 20 x 20 -> 40 x 40\n",
        "    deconv1 = Conv2DTranspose(start_neurons * 1, (3, 3), strides=(2, 2), padding=\"same\", activation = tf.keras.layers.LeakyReLU(alpha=0.1))(uconv2)\n",
        "    uconv1 = concatenate([deconv1, convc1, convd1])\n",
        "    uconv1 = Dropout(0.25)(uconv1)\n",
        "    uconv1 = Conv2D(start_neurons * 1, (3, 3), activation=tf.keras.layers.LeakyReLU(alpha=0.1), padding=\"same\", kernel_initializer='glorot_uniform')(uconv1)\n",
        "    uconv1 = Conv2D(start_neurons * 1, (3, 3), activation=tf.keras.layers.LeakyReLU(alpha=0.1), padding=\"same\", kernel_initializer='glorot_uniform')(uconv1)\n",
        "    uconv1 = BatchNormalization()(uconv1)\n",
        "    uconv1 = Dropout(0.25)(uconv1)\n",
        "    output_layer = Conv2D(1, (1,1),activation=tf.keras.layers.LeakyReLU(alpha=0.1), padding=\"same\", kernel_initializer='glorot_uniform')(uconv1)\n",
        "    \n",
        "    return output_layer\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXRyvpMVr21K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_layer_c = Input((40, 40, 9))\n",
        "input_layer_d = Input((40, 40, 4))\n",
        "output_layer = build_model(input_layer_c, input_layer_d,  32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqMt4p1N09nx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Model(inputs = [input_layer_c, input_layer_d], outputs = [output_layer])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_NhbS4s09p_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "def mae(y_true, y_pred) :\n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "    y_true = y_true.reshape(1, -1)[0]\n",
        "    y_pred = y_pred.reshape(1, -1)[0]\n",
        "    over_threshold = y_true >= 0.1\n",
        "    return np.mean(np.abs(y_true[over_threshold] - y_pred[over_threshold]))\n",
        "def fscore(y_true, y_pred):\n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "    y_true = y_true.reshape(1, -1)[0]\n",
        "    y_pred = y_pred.reshape(1, -1)[0]\n",
        "    remove_NAs = y_true >= 0\n",
        "    y_true = np.where(y_true[remove_NAs] >= 0.1, 1, 0)\n",
        "    y_pred = np.where(y_pred[remove_NAs] >= 0.1, 1, 0)\n",
        "    return(f1_score(y_true, y_pred))\n",
        "def maeOverFscore(y_true, y_pred):\n",
        "    return mae(y_true, y_pred) / (fscore(y_true, y_pred) + 1e-07)\n",
        "def fscore_keras(y_true, y_pred):\n",
        "    score = tf.py_function(func=fscore, inp=[y_true, y_pred], Tout=tf.float32, name='fscore_keras')\n",
        "    return score\n",
        "def maeOverFscore_keras(y_true, y_pred):\n",
        "    score = tf.py_function(func=maeOverFscore, inp=[y_true, y_pred], Tout=tf.float32,  name='custom_mse') \n",
        "    return score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sv7Hk_LM1DNf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(loss=\"mae\", optimizer=opt, metrics=[maeOverFscore_keras, fscore_keras])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zxke2xER1EHj",
        "colab_type": "code",
        "outputId": "de617ffb-b5f3-4238-87f5-b69be863905f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        }
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "kf = KFold(n_splits=5)\n",
        "model_history = []\n",
        "fold_number = 0\n",
        "\n",
        "for train_index, test_index in kf.split(train):\n",
        "    print(f'\\nFold {fold_number}')\n",
        "    fold_number = fold_number + 1\n",
        "    \n",
        "#-----------------------------------------------------------------------------\n",
        "    def trainGenerator():\n",
        "        print(1)\n",
        "        for file in train[train_index]:\n",
        "            target= file[:,:,-1:].reshape(40,40,1)\n",
        "            cutoff_labels = np.where(target < 0, 0, target)\n",
        "            feature_c = file[:,:,:9]\n",
        "            feature_d = file[:,:,9:13]\n",
        "            if (cutoff_labels > 0).sum() < 50:\n",
        "                continue\n",
        "            yield (feature_c, feature_d), cutoff_labels\n",
        "\n",
        "    train_dataset = tf.data.Dataset.from_generator(trainGenerator,\n",
        "                                                ((tf.float32, tf.float32), tf.float32),\n",
        "                                                ((tf.TensorShape([40,40,9]), tf.TensorShape([40, 40, 4])), tf.TensorShape([40,40,1])))\n",
        "    \n",
        "    train_dataset = train_dataset.batch(512).prefetch(1)\n",
        "#-----------------------------------------------------------------------------\n",
        "    def testGenerator():\n",
        "        print(2)\n",
        "        for file in train[test_index]:\n",
        "            target= file[:,:,-1:].reshape(40,40,1)\n",
        "            cutoff_labels = np.where(target < 0, 0, target)\n",
        "            feature_c = file[:,:,:9]\n",
        "            feature_d = file[:,:,9:13]\n",
        "            if (cutoff_labels > 0).sum() < 50:\n",
        "                continue\n",
        "            yield (feature_c, feature_d), cutoff_labels\n",
        "    test_dataset = tf.data.Dataset.from_generator(testGenerator,\n",
        "                                                ((tf.float32, tf.float32), tf.float32),\n",
        "                                                ((tf.TensorShape([40,40,9]), tf.TensorShape([40, 40, 4])),tf.TensorShape([40,40,1])))\n",
        "    test_size = len(test_index)\n",
        "    test_dataset = test_dataset.batch(test_size).prefetch(1)\n",
        "#-----------------------------------------------------------------------------\n",
        "    history = model.fit(train_dataset, epochs = 20, verbose=1, validation_data = test_dataset)\n",
        "    model.save(\"/content/drive/My Drive/2020 Kaggle Study/Theos/Model/three_leg_CNN_5.h5\")\n",
        "    #history = model.fit(train_dataset, epochs = 5, ,batch_size = 11, verbose=1)\n",
        "    model_history.append(history)\n",
        "    #del data\n",
        "    del train_dataset\n",
        "    del test_dataset\n",
        "    gc.collect()"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Fold 0\n",
            "Epoch 1/20\n",
            "1\n",
            "      4/Unknown - 8s 2s/step - loss: 0.4111 - maeOverFscore_keras: 511.9468 - fscore_keras: 0.0015"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-5d66f4c19630>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m#-----------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/My Drive/2020 Kaggle Study/Theos/Model/three_leg_CNN_5.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;31m#history = model.fit(train_dataset, epochs = 5, ,batch_size = 11, verbose=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kucgrytn8-yj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "outputId": "43900e94-0878-444a-c261-a94c85d52878"
      },
      "source": [
        "model = tf.keras.models.load_model('/content/drive/My Drive/2020 Kaggle Study/Theos/Model/three_leg_CNN_4.h5', custom_objects={\"maeOverFscore\": maeOverFscore,\"fscore\":fscore })"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-152dbd2b52b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/2020 Kaggle Study/Theos/Model/three_leg_CNN_4.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"maeOverFscore\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmaeOverFscore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"fscore\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfscore\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    182\u001b[0m     if (h5py is not None and (\n\u001b[1;32m    183\u001b[0m         isinstance(filepath, h5py.File) or h5py.is_hdf5(filepath))):\n\u001b[0;32m--> 184\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mhdf5_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model_from_hdf5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36mload_model_from_hdf5\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    192\u001b[0m       \u001b[0;31m# Compile model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m       model.compile(**saving_utils.compile_args_from_training_config(\n\u001b[0;32m--> 194\u001b[0;31m           training_config, custom_objects))\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m       \u001b[0;31m# Set optimizer weights.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/saving_utils.py\u001b[0m in \u001b[0;36mcompile_args_from_training_config\u001b[0;34m(training_config, custom_objects)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0mmetrics_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'metrics'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmetrics_config\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m       \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deserialize_nested_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_deserialize_metric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;31m# Recover weighted metrics.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/saving_utils.py\u001b[0m in \u001b[0;36m_deserialize_nested_config\u001b[0;34m(deserialize_fn, config)\u001b[0m\n\u001b[1;32m    260\u001b[0m     }\n\u001b[1;32m    261\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_deserialize_nested_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeserialize_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m   \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Saved configuration not understood.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/saving_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    260\u001b[0m     }\n\u001b[1;32m    261\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_deserialize_nested_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeserialize_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m   \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Saved configuration not understood.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/saving_utils.py\u001b[0m in \u001b[0;36m_deserialize_nested_config\u001b[0;34m(deserialize_fn, config)\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_is_single_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeserialize_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     return {\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/saving_utils.py\u001b[0m in \u001b[0;36m_deserialize_metric\u001b[0;34m(metric_config)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;31m# case handling for these in compile, based on model output shape.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmetric_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mmetrics_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/metrics.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m   3347\u001b[0m       \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3348\u001b[0m       \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3349\u001b[0;31m       printable_module_name='metric function')\n\u001b[0m\u001b[1;32m   3350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    390\u001b[0m       \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule_objects\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unknown '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mprintable_module_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m':'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mobject_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m     \u001b[0;31m# Classes passed by name are instantiated with no args, functions are\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m     \u001b[0;31m# returned as-is.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unknown metric function:maeOverFscore_keras"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFIVaK6rSRZs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_tempandland = np.load(\"/content/drive/My Drive/2020 Kaggle Study/data/yeonung/test.npy\")[:,:,:,:10]\n",
        "test_DEM = np.load(\"/content/drive/My Drive/2020 Kaggle Study/data/DEM_DPR/dem_test_DPR.npy\")\n",
        "test_day = np.load(\"/content/drive/My Drive/2020 Kaggle Study/data/orbit_no_shuffle/test_days.npy\")\n",
        "test_times = np.load(\"/content/drive/My Drive/2020 Kaggle Study/data/orbit_no_shuffle/test_times.npy\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdTi7FZBVEKC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = np.concatenate((test_tempandland, test_DEM, test_day, test_times), axis = 3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raJnv0_aTVCH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d5ecc03e-6ff5-458c-c62a-a0b4c1dc43a6"
      },
      "source": [
        "print(test.shape)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2416, 40, 40, 15)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZ1bQvRSVNLl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred=model.predict([test[:,:,:,:9],test[:,:,:,9:11],test[:,:,:,11:]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vo6hXSOVQkj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submission = pd.read_csv('/content/drive/My Drive/2020 Kaggle Study/data/sample_submission.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUTWSL8LVn79",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submission.iloc[:,1:] = pred.reshape(-1, 1600)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvlSMMFXVp38",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submission.to_csv('/content/drive/My Drive/2020 Kaggle Study/Theos/Submission/three_leg_CNN_all.csv', index = False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}