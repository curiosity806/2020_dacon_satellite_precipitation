{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMB+ZQAoNF3LR08aBHFRfU1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/curiosity806/2020_dacon_satellite_precipitation/blob/master/pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBewILWFjgrI",
        "colab_type": "text"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19l2vB3SqhaP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5R92vw85pL5g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# install pytorch tpu version\n",
        "# VERSION = \"20200325\"  #@param [\"1.5\" , \"20200325\", \"nightly\"]\n",
        "# !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "# !python pytorch-xla-env-setup.py --version $VERSION"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vfLpM4EpOH0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import random\n",
        "import os.path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "from torchsummary import summary\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eTOpvbfrA05",
        "colab_type": "code",
        "outputId": "e806391b-370f-41c7-dc8e-f147fe1a4764",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "torch.__version__  # should be 1.5.0a0+d6149a7"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.5.0a0+d6149a7'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqLWAFJboAkO",
        "colab_type": "text"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rI9UlJxL9GHD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.isfile('train.npy'):\n",
        "    !cp '/content/drive/My Drive/2020 Kaggle Study/data/train.npy' train.npy\n",
        "if not os.path.isfile('test.npy'):\n",
        "    !cp '/content/drive/My Drive/2020 Kaggle Study/data/test.npy' test.npy\n",
        "if not os.path.isfile('gmi_preci.npy'):\n",
        "    !cp '/content/drive/My Drive/2020 Kaggle Study/data/gmi_preci/near2.npy' gmi_preci.npy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96Z2wskO9dEX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = np.load('train.npy')  # float32\n",
        "test = np.load('test.npy')  # float64\n",
        "gmi_preci = np.load('gmi_preci.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOekBm82oDpM",
        "colab_type": "text"
      },
      "source": [
        "## Data preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzaPnfektQjU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train-valid split\n",
        "train, valid, train_preci, valid_preci = train_test_split(train, gmi_preci, test_size=0.025, random_state=7777)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1RkguZBsWIF",
        "colab_type": "code",
        "outputId": "e9a1cdcc-f495-494c-bcd6-ccb4fcf1992c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# 위경도 제거\n",
        "train = np.concatenate((train[:,:,:,:10], train[:,:,:,-1:]), axis=3)\n",
        "train.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(74436, 40, 40, 11)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZ5qJ0QQSKsh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -9999를 포함한 이미지 제거\n",
        "is_valid = (train[:,:,:,-1].reshape(-1, 1600) < 0).sum(axis=1) == 0\n",
        "train = train[is_valid]\n",
        "train_preci = train_preci[is_valid]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUwWlFL9tPVl",
        "colab_type": "code",
        "outputId": "a0946199-db92-4326-9337-5d47709fdd67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# fit scaler\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(train[:,:,:,:9].reshape(-1, 9))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StandardScaler(copy=True, with_mean=True, with_std=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uhfLqW39xhJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# gmi 강수량으로 대치\n",
        "train[:, :, :, -1] = train_preci.reshape(-1, 40, 40)\n",
        "valid_preci = valid[:, :, :, -1].copy()\n",
        "valid[:, :, :, -1] = valid_preci.reshape(-1, 40, 40)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-fnxrCEvDMk",
        "colab_type": "code",
        "outputId": "080b471a-ad37-4ad5-c874-aa2fd729863e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "# stratefied oversample\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "rain = train[:,:,:,-1].reshape(-1, 1600).sum(axis=1)\n",
        "st = np.where(rain < 1, 0,\n",
        "                  np.where(rain < 10, 1,\n",
        "                           np.where(rain < 100, 2,\n",
        "                                    np.where(rain < 1000, 3,\n",
        "                                             4))))\n",
        "rus = RandomOverSampler(random_state=7777)\n",
        "train = train.reshape(-1, 17600)\n",
        "train, _ = rus.fit_resample(train, st)\n",
        "train = train.reshape(-1, 40, 40, 11)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwlYqyfCtZTd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# scale sensor values and land type\n",
        "def scale(data):\n",
        "    # normalize sensor data\n",
        "    data[:,:,:,:9] = scaler.transform(data[:,:,:,:9].reshape(-1, 9)).reshape(-1, 40, 40, 9)\n",
        "\n",
        "    # ocean 0.0, land 1.0, coastal 0.7, inland water 0.3\n",
        "    land_type_data = data[:,:,:,9]\n",
        "    data[:,:,:,9] = np.where(land_type_data//100 == 2, 0.7,\n",
        "                             np.where(land_type_data//100 == 3, 0.3,\n",
        "                                      land_type_data//100))\n",
        "    return data\n",
        "\n",
        "train = scale(train)\n",
        "valid = scale(valid)\n",
        "test = scale(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjpCwoktXP_u",
        "colab_type": "code",
        "outputId": "d0db9795-e4dc-485b-96af-6cde8a8c9b54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# pytorch is channel first\n",
        "train = train.transpose((0, 3, 1, 2))\n",
        "valid = valid.transpose((0, 3, 1, 2))\n",
        "test = test.transpose((0, 3, 1, 2))\n",
        "train.shape, valid.shape, test.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((106595, 11, 40, 40), (1909, 15, 40, 40), (2416, 14, 40, 40))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LqO5tzEoP51",
        "colab_type": "text"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGQJSqzO1kuK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def r(a, n=1):\n",
        "    return np.rot90(a, n, (1, 2))\n",
        "def f_ud(a):\n",
        "    return np.flip(a, 1)\n",
        "def f_lr(a):\n",
        "    return np.flip(a, 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRzlHv9h1n3r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf_list = [lambda x: x,              # abcd\n",
        "           lambda x: r(f_lr(x)),     # acbd\n",
        "           lambda x: f_lr(x),        # badc\n",
        "           lambda x: r(x),           # bdac\n",
        "           lambda x: r(x, 3),        # cadb\n",
        "           lambda x: f_ud(x),        # cdab\n",
        "           lambda x: f_ud(r(x, 3)),  # dbca\n",
        "           lambda x: r(x, 2)]        # dcba"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69TaaeZZ1px_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MapDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, is_train, transform=None):\n",
        "        self.data = data\n",
        "        self.is_train = is_train\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.transform:\n",
        "            rand_num = random.randint(0, 7)\n",
        "            sample = self.transform[rand_num](self.data[idx])\n",
        "        else:\n",
        "            sample = self.data[idx]\n",
        "\n",
        "        sample = torch.from_numpy(sample.copy())\n",
        "\n",
        "        if self.is_train:\n",
        "            return sample[:10], sample[-1:]\n",
        "        else:\n",
        "            return sample[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyMYGNpSn74D",
        "colab_type": "text"
      },
      "source": [
        "## Define metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S41kNGgzn6_S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mae(y_true, y_pred):\n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "    y_true = y_true.reshape(1, -1)[0]\n",
        "    y_pred = y_pred.reshape(1, -1)[0]\n",
        "    over_threshold = y_true >= 0.1\n",
        "    return np.mean(np.abs(y_true[over_threshold] - y_pred[over_threshold]))\n",
        "\n",
        "def fscore(y_true, y_pred):\n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "    y_true = y_true.reshape(1, -1)[0]\n",
        "    y_pred = y_pred.reshape(1, -1)[0]\n",
        "    remove_NAs = y_true >= 0\n",
        "    y_true = np.where(y_true[remove_NAs] >= 0.1, 1, 0)\n",
        "    y_pred = np.where(y_pred[remove_NAs] >= 0.1, 1, 0)\n",
        "    return (f1_score(y_true, y_pred))\n",
        "\n",
        "def maeOverFscore(y_true, y_pred):\n",
        "    return mae(y_true, y_pred) / (fscore(y_true, y_pred) + 1e-07)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seFeikIyQAgf",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBkr7Yy9YMjy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DoubleConv(nn.Module):\n",
        "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
        "        super().__init__()\n",
        "        if not mid_channels:\n",
        "            mid_channels = out_channels\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(mid_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "\n",
        "class Down(nn.Module):\n",
        "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(in_channels, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.maxpool_conv(x)\n",
        "\n",
        "\n",
        "class Up(nn.Module):\n",
        "    \"\"\"Upscaling then double conv\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
        "        super().__init__()\n",
        "\n",
        "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
        "        if bilinear:\n",
        "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
        "        else:\n",
        "            self.up = nn.ConvTranspose2d(in_channels , in_channels // 2, kernel_size=2, stride=2)\n",
        "            self.conv = DoubleConv(in_channels, out_channels)\n",
        "\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "        # input is CHW\n",
        "        diffY = x2.size()[2] - x1.size()[2]\n",
        "        diffX = x2.size()[3] - x1.size()[3]\n",
        "\n",
        "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
        "                        diffY // 2, diffY - diffY // 2])\n",
        "        # if you have padding issues, see\n",
        "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
        "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class OutConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(OutConv, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BUzS5WFCImT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
        "        super(UNet, self).__init__()\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.bilinear = bilinear\n",
        "\n",
        "        self.inc = DoubleConv(n_channels, 64)\n",
        "        self.down1 = Down(64, 128)\n",
        "        self.down2 = Down(128, 256)\n",
        "        self.down3 = Down(256, 512)\n",
        "        factor = 2 if bilinear else 1\n",
        "        self.down4 = Down(512, 1024 // factor)\n",
        "        self.up1 = Up(1024, 512 // factor, bilinear)\n",
        "        self.up2 = Up(768, 256 // factor, bilinear)\n",
        "        self.up3 = Up(256, 128 // factor, bilinear)\n",
        "        self.up4 = Up(128, 64, bilinear)\n",
        "        self.outc = OutConv(64, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        # x5 = self.down4(x4)\n",
        "        # x = self.up1(x5, x4)\n",
        "        # x = self.up2(x, x3)\n",
        "        x = self.up2(x4, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        logits = self.outc(x)\n",
        "        return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YlWf2yOaFGm",
        "colab_type": "code",
        "outputId": "f4eb0001-af8e-4fb0-d9d0-3ae1ef0a6ae4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "summary(UNet(10, 1), (10, 40, 40))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 40, 40]           5,824\n",
            "       BatchNorm2d-2           [-1, 64, 40, 40]             128\n",
            "              ReLU-3           [-1, 64, 40, 40]               0\n",
            "            Conv2d-4           [-1, 64, 40, 40]          36,928\n",
            "       BatchNorm2d-5           [-1, 64, 40, 40]             128\n",
            "              ReLU-6           [-1, 64, 40, 40]               0\n",
            "        DoubleConv-7           [-1, 64, 40, 40]               0\n",
            "         MaxPool2d-8           [-1, 64, 20, 20]               0\n",
            "            Conv2d-9          [-1, 128, 20, 20]          73,856\n",
            "      BatchNorm2d-10          [-1, 128, 20, 20]             256\n",
            "             ReLU-11          [-1, 128, 20, 20]               0\n",
            "           Conv2d-12          [-1, 128, 20, 20]         147,584\n",
            "      BatchNorm2d-13          [-1, 128, 20, 20]             256\n",
            "             ReLU-14          [-1, 128, 20, 20]               0\n",
            "       DoubleConv-15          [-1, 128, 20, 20]               0\n",
            "             Down-16          [-1, 128, 20, 20]               0\n",
            "        MaxPool2d-17          [-1, 128, 10, 10]               0\n",
            "           Conv2d-18          [-1, 256, 10, 10]         295,168\n",
            "      BatchNorm2d-19          [-1, 256, 10, 10]             512\n",
            "             ReLU-20          [-1, 256, 10, 10]               0\n",
            "           Conv2d-21          [-1, 256, 10, 10]         590,080\n",
            "      BatchNorm2d-22          [-1, 256, 10, 10]             512\n",
            "             ReLU-23          [-1, 256, 10, 10]               0\n",
            "       DoubleConv-24          [-1, 256, 10, 10]               0\n",
            "             Down-25          [-1, 256, 10, 10]               0\n",
            "        MaxPool2d-26            [-1, 256, 5, 5]               0\n",
            "           Conv2d-27            [-1, 512, 5, 5]       1,180,160\n",
            "      BatchNorm2d-28            [-1, 512, 5, 5]           1,024\n",
            "             ReLU-29            [-1, 512, 5, 5]               0\n",
            "           Conv2d-30            [-1, 512, 5, 5]       2,359,808\n",
            "      BatchNorm2d-31            [-1, 512, 5, 5]           1,024\n",
            "             ReLU-32            [-1, 512, 5, 5]               0\n",
            "       DoubleConv-33            [-1, 512, 5, 5]               0\n",
            "             Down-34            [-1, 512, 5, 5]               0\n",
            "         Upsample-35          [-1, 512, 10, 10]               0\n",
            "           Conv2d-36          [-1, 384, 10, 10]       2,654,592\n",
            "      BatchNorm2d-37          [-1, 384, 10, 10]             768\n",
            "             ReLU-38          [-1, 384, 10, 10]               0\n",
            "           Conv2d-39          [-1, 128, 10, 10]         442,496\n",
            "      BatchNorm2d-40          [-1, 128, 10, 10]             256\n",
            "             ReLU-41          [-1, 128, 10, 10]               0\n",
            "       DoubleConv-42          [-1, 128, 10, 10]               0\n",
            "               Up-43          [-1, 128, 10, 10]               0\n",
            "         Upsample-44          [-1, 128, 20, 20]               0\n",
            "           Conv2d-45          [-1, 128, 20, 20]         295,040\n",
            "      BatchNorm2d-46          [-1, 128, 20, 20]             256\n",
            "             ReLU-47          [-1, 128, 20, 20]               0\n",
            "           Conv2d-48           [-1, 64, 20, 20]          73,792\n",
            "      BatchNorm2d-49           [-1, 64, 20, 20]             128\n",
            "             ReLU-50           [-1, 64, 20, 20]               0\n",
            "       DoubleConv-51           [-1, 64, 20, 20]               0\n",
            "               Up-52           [-1, 64, 20, 20]               0\n",
            "         Upsample-53           [-1, 64, 40, 40]               0\n",
            "           Conv2d-54           [-1, 64, 40, 40]          73,792\n",
            "      BatchNorm2d-55           [-1, 64, 40, 40]             128\n",
            "             ReLU-56           [-1, 64, 40, 40]               0\n",
            "           Conv2d-57           [-1, 64, 40, 40]          36,928\n",
            "      BatchNorm2d-58           [-1, 64, 40, 40]             128\n",
            "             ReLU-59           [-1, 64, 40, 40]               0\n",
            "       DoubleConv-60           [-1, 64, 40, 40]               0\n",
            "               Up-61           [-1, 64, 40, 40]               0\n",
            "           Conv2d-62            [-1, 1, 40, 40]              65\n",
            "          OutConv-63            [-1, 1, 40, 40]               0\n",
            "================================================================\n",
            "Total params: 8,271,617\n",
            "Trainable params: 8,271,617\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.06\n",
            "Forward/backward pass size (MB): 22.63\n",
            "Params size (MB): 31.55\n",
            "Estimated Total Size (MB): 54.25\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2875: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lurdnBaxJE29",
        "colab_type": "text"
      },
      "source": [
        "## 강수량 보정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tu8n0BdhJF2t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dr = [(-1, -1), (-1, 0), (-1, 1),\n",
        "      (0, -1), (0, 0), (0, 1),\n",
        "      (1, -1), (1, 0), (1, 1)]\n",
        "\n",
        "def get_dist(p1, p2):  # p1, p2: shape=(-1, 2).\n",
        "    x1, y1 = np.deg2rad(p1[:,0]), np.deg2rad(p1[:,1])\n",
        "    x2, y2 = np.deg2rad(p2[:,0]), np.deg2rad(p2[:,1])\n",
        "    dlon = x2 - x1\n",
        "    dlat = y2 - y1\n",
        "    a = np.sin(dlat/2)**2 + np.cos(y1) * np.cos(y2) * np.sin(dlon/2)**2 \n",
        "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))    \n",
        "    return 6373.0 * c  # km, shape=(-1).\n",
        "\n",
        "# value: (40, 40)\n",
        "# ori_ll, tgt_ll: (40, 40, 2)\n",
        "def compen_ll(value, ori_ll, tgt_ll):  # ori_ll에서의 value를 tgt_ll에 대한 값으로 바꿈\n",
        "    ret = np.empty_like(value)\n",
        "    n, m = value.shape[0], value.shape[1]\n",
        "    for i in range(n):\n",
        "        for j in range(m):\n",
        "            nears = []  # (row, col, value)\n",
        "            for k in range(9):\n",
        "                ii = i + dr[k][0]\n",
        "                jj = j + dr[k][1]\n",
        "                if ii >= 0 and ii < n and jj >= 0 and jj < m:\n",
        "                    nears.append((ori_ll[ii, jj][0], ori_ll[ii, jj][1],\n",
        "                                  tgt_ll[i, j][0], tgt_ll[i, j][1],\n",
        "                                  value[ii, jj]))\n",
        "            nears = np.array(nears)  # shape=(-1, 5)\n",
        "            dists = get_dist(nears[:, 0:2], nears[:, 2:4]).reshape(-1, 1)\n",
        "            values = nears[:, 4].reshape(-1, 1)\n",
        "            nears = np.concatenate((dists, values), 1)\n",
        "            nears = nears[np.argsort(nears[:, 0])]  # sort by dist\n",
        "            nears = nears[:2, :]  # 가까운 점 2개만 고려\n",
        "\n",
        "            weights = 1 / (nears[:, 0] ** 2 + sys.float_info.epsilon)\n",
        "            weighted_sum = (weights * nears[:, 1]).sum()\n",
        "            ret[i, j] = weighted_sum / weights.sum()\n",
        "    return ret"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2x3plaQzJPfS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from multiprocessing import Process, Manager\n",
        "\n",
        "def proc_func(splitted, dpr_preci, proc_id):\n",
        "    part = splitted[proc_id]\n",
        "    arr = np.empty_like(part[:, :, :, 14])  # shape=(-1, 40, 40)\n",
        "    for i in range(part.shape[0]):\n",
        "        arr[i, :, :] = compen_ll(part[i, :, :, 14], part[i, :, :, 10:12], part[i, :, :, 12:14])\n",
        "    dpr_preci[proc_id] = arr\n",
        "\n",
        "def gmi2dpr(test, gmi_preci):\n",
        "    n_procs = 4\n",
        "    procs = []\n",
        "    manager = Manager()\n",
        "    dpr_preci = manager.list([None] * n_procs)\n",
        "\n",
        "    data = np.concatenate((test, gmi_preci), axis=3)\n",
        "    n_imgs = data.shape[0]  # split data into n_procs arrays\n",
        "    splitted = np.split(data, np.arange(n_imgs // n_procs + n_imgs % n_procs, n_imgs, n_imgs // n_procs))\n",
        "\n",
        "    for proc_id in range(n_procs):\n",
        "        proc = Process(target=proc_func, args=(splitted, dpr_preci, proc_id))\n",
        "        proc.start()\n",
        "        procs.append(proc)\n",
        "\n",
        "    for proc in procs:\n",
        "        proc.join()\n",
        "\n",
        "    dpr_preci = np.concatenate(dpr_preci)\n",
        "    return dpr_preci"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOzgiKhuoiHM",
        "colab_type": "text"
      },
      "source": [
        "## TPU training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3OJPWS9Dvvl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def map_fn(index, flags):\n",
        "    device = xm.xla_device()  # TPU\n",
        "\n",
        "    train_dataset = MapDataset(train, True, tf_list)\n",
        "    valid_dataset = MapDataset(valid, False)\n",
        "    test_dataset = MapDataset(test, False)\n",
        "\n",
        "    train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "        train_dataset,\n",
        "        num_replicas=xm.xrt_world_size(),\n",
        "        rank=xm.get_ordinal(),\n",
        "        shuffle=True)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                               batch_size=flags['batch_size'],\n",
        "                                               sampler=train_sampler,\n",
        "                                               num_workers=flags['num_workers'],\n",
        "                                               drop_last=True)  # small batch is not good for bn\n",
        "    valid_loader = torch.utils.data.DataLoader(valid_dataset,\n",
        "                                               batch_size=len(valid),\n",
        "                                               num_workers=1,\n",
        "                                               drop_last=False)\n",
        "    test_loader = torch.utils.data.DataLoader(test_dataset,\n",
        "                                              batch_size=len(test),\n",
        "                                              num_workers=1,\n",
        "                                              drop_last=False)\n",
        "\n",
        "    # min_score 초기화\n",
        "    min_score = 5.0\n",
        "    cnt = 0\n",
        "\n",
        "    # net, loss_fn, optimizer 생성\n",
        "    net = UNet(10, 1).to(device).train()  # Training mode\n",
        "    loss_fn = torch.nn.L1Loss()\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=0.0002)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.25, patience=5, verbose=True)\n",
        "\n",
        "    # Training\n",
        "    for epoch in range(flags['num_epochs']):\n",
        "        # Training mode\n",
        "        net.train()\n",
        "\n",
        "        # 병렬 연산을 위한 loader\n",
        "        para_train_loader = pl.ParallelLoader(train_loader, [device]).per_device_loader(device)\n",
        "\n",
        "        # Update parameters\n",
        "        for batch_num, batch in enumerate(para_train_loader):\n",
        "            data, targets = batch\n",
        "\n",
        "            # Inference -> loss 계산 -> gradient 초기화 -> backpropagation\n",
        "            output = net(data)  # Acquires the network's best guesses\n",
        "            loss = loss_fn(output, targets)  # Computes loss\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()  # Updates model\n",
        "\n",
        "            xm.optimizer_step(optimizer)  # Note: barrier=True not needed when using ParallelLoader\n",
        "\n",
        "        # Main core에서만 실행\n",
        "        if xm.is_master_ordinal():\n",
        "            print('epoch', epoch, end=': ')\n",
        "\n",
        "            # Evaluation mode\n",
        "            net.eval()\n",
        "\n",
        "            # Prediction\n",
        "            with torch.no_grad():\n",
        "                for batch_num, batch in enumerate(valid_loader):\n",
        "                    output = net(batch.to(device))  # (-1, 1, 40, 40)\n",
        "\n",
        "            # Numpy로 변환\n",
        "            output = output.cpu().numpy()\n",
        "            output = np.where(output < 0.1, 0.099, output)\n",
        "\n",
        "            # Score 계산\n",
        "            mae_score = mae(valid[:,-1,:,:], output)\n",
        "            f_score = fscore(valid[:,-1,:,:], output)\n",
        "            mae_over_fscore = maeOverFscore(valid[:,-1,:,:], output)\n",
        "            print(f'mae={mae_score}, fscore={f_score}, mae/fscore={mae_over_fscore}')\n",
        "\n",
        "            # Best model 저장\n",
        "            if mae_over_fscore < min_score:\n",
        "                min_score = mae_over_fscore\n",
        "                torch.save(net.state_dict(), 'best_model.pt')\n",
        "                print('model saved.')\n",
        "                cnt = 0\n",
        "            else:\n",
        "                cnt += 1\n",
        "\n",
        "            scheduler.step(mae_over_fscore)\n",
        "\n",
        "        xm.rendezvous('validation')\n",
        "\n",
        "        if cnt >= 10:\n",
        "            break\n",
        "\n",
        "    # Main core에서만 실행\n",
        "    if xm.is_master_ordinal():\n",
        "        # Best model 불러오기\n",
        "        net.load_state_dict(torch.load('best_model.pt'))\n",
        "        print('model loaded.')\n",
        "\n",
        "        # Evaluation mode로 전환\n",
        "        net.eval()\n",
        "\n",
        "        # Final validation\n",
        "        with torch.no_grad():\n",
        "            for batch_num, batch in enumerate(valid_loader):\n",
        "                output = net(batch.to(device))  # (-1, 1, 40, 40)\n",
        "\n",
        "        # Numpy로 변환\n",
        "        output = output.cpu().numpy()\n",
        "\n",
        "        # DPR 강수량으로 보정\n",
        "        output = gmi2dpr(valid.transpose((0, 2, 3, 1))[:, :, :, :-1], output.transpose((0, 2, 3, 1)))\n",
        "\n",
        "        # Score 계산\n",
        "        mae_score = mae(valid_preci, output)\n",
        "        f_score = fscore(valid_preci, output)\n",
        "        mae_over_fscore = maeOverFscore(valid_preci, output)\n",
        "        print(f'final val: mae={mae_score}, fscore={f_score}, mae/fscore={mae_over_fscore}')\n",
        "\n",
        "        # Prediction\n",
        "        with torch.no_grad():\n",
        "            for batch_num, batch in enumerate(test_loader):\n",
        "                output = net(batch.to(device))  # (-1, 1, 40, 40)\n",
        "\n",
        "        # Numpy로 변환\n",
        "        output = output.cpu().numpy()\n",
        "\n",
        "        # 강수량 보정\n",
        "        output = gmi2dpr(test.transpose((0, 2, 3, 1)), output.transpose((0, 2, 3, 1)))\n",
        "        output = np.where(output < 0.1, 0.099, output)\n",
        "\n",
        "        # Submission file 올리기\n",
        "        submission = pd.read_csv('/content/drive/My Drive/2020 Kaggle Study/data/sample_submission.csv')\n",
        "        submission.iloc[:,1:] = output.reshape(-1, 1600)\n",
        "        submission.to_csv('/content/drive/My Drive/2020 Kaggle Study/submission/unet_compen2.csv', index=False)\n",
        "        print('submission uploaded.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNS0iV6K8CtU",
        "colab_type": "code",
        "outputId": "13d2665e-af03-4fe6-d695-f041f2f21c38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Configures training (and evaluation) parameters\n",
        "flags = {'batch_size': 1024,\n",
        "         'num_workers': 8,\n",
        "         'num_epochs': 50,\n",
        "         'seed': 7777}\n",
        "\n",
        "xmp.spawn(map_fn, args=(flags,), nprocs=8, start_method='fork')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2875: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2875: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2875: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2875: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2875: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2875: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2875: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2875: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 0: mae=1.9966132640838623, fscore=0.02989710553603183, mae/fscore=66.78260487180191\n",
            "epoch 1: mae=1.8264210224151611, fscore=0.489035196114291, mae/fscore=3.7347427413262078\n",
            "model saved.\n",
            "epoch 2: mae=1.6421180963516235, fscore=0.5314953726722248, mae/fscore=3.089618220255893\n",
            "model saved.\n",
            "epoch 3: mae=1.5293227434158325, fscore=0.5747055578263657, mae/fscore=2.661053919670794\n",
            "model saved.\n",
            "epoch 4: mae=1.5088778734207153, fscore=0.6135310427734905, mae/fscore=2.4593337945320535\n",
            "model saved.\n",
            "epoch 5: mae=1.4949978590011597, fscore=0.6496391831721172, mae/fscore=2.301273795668959\n",
            "model saved.\n",
            "epoch 6: mae=1.4797327518463135, fscore=0.6273305048729013, mae/fscore=2.358776600969613\n",
            "epoch 7: mae=1.4561376571655273, fscore=0.673416737619117, mae/fscore=2.1623125170343833\n",
            "model saved.\n",
            "epoch 8: mae=1.4481595754623413, fscore=0.6718689817739227, mae/fscore=2.1554192844218907\n",
            "model saved.\n",
            "epoch 9: mae=1.4325371980667114, fscore=0.6842858418253472, mae/fscore=2.093477463887959\n",
            "model saved.\n",
            "epoch 10: mae=1.4176286458969116, fscore=0.6921328543148325, mae/fscore=2.0482027868479022\n",
            "model saved.\n",
            "epoch 11: mae=1.4100000858306885, fscore=0.6932745002511682, mae/fscore=2.0338262577625894\n",
            "model saved.\n",
            "epoch 12: mae=1.3996258974075317, fscore=0.6962494036756299, mae/fscore=2.010236115097602\n",
            "model saved.\n",
            "epoch 13: mae=1.407758355140686, fscore=0.6867742530053862, mae/fscore=2.0498120656081715\n",
            "epoch 14: mae=1.3887099027633667, fscore=0.7003236578474185, mae/fscore=1.9829541511369504\n",
            "model saved.\n",
            "epoch 15: mae=1.3814903497695923, fscore=0.7065600752410064, mae/fscore=1.9552338189714253\n",
            "model saved.\n",
            "epoch 16: mae=1.376257061958313, fscore=0.6931631117129725, mae/fscore=1.9854733181197715\n",
            "epoch 17: mae=1.3630733489990234, fscore=0.7130722570597156, mae/fscore=1.9115498385318481\n",
            "model saved.\n",
            "epoch 18: mae=1.3567817211151123, fscore=0.7085940394352283, mae/fscore=1.9147515419708523\n",
            "epoch 19: mae=1.3583937883377075, fscore=0.7069239185711425, mae/fscore=1.9215555740818324\n",
            "epoch 20: mae=1.3450700044631958, fscore=0.7204439423948779, mae/fscore=1.8670013565411072\n",
            "model saved.\n",
            "epoch 21: mae=1.341554045677185, fscore=0.7195702893495074, mae/fscore=1.8643819500270875\n",
            "model saved.\n",
            "epoch 22: mae=1.3336478471755981, fscore=0.7228051600401536, mae/fscore=1.845099808904975\n",
            "model saved.\n",
            "epoch 23: mae=1.3313193321228027, fscore=0.7243793957862013, mae/fscore=1.8378755056807106\n",
            "model saved.\n",
            "epoch 24: mae=1.3274837732315063, fscore=0.7278410598730275, mae/fscore=1.8238646650088972\n",
            "model saved.\n",
            "epoch 25: mae=1.3249167203903198, fscore=0.727683484323731, mae/fscore=1.820731907291305\n",
            "model saved.\n",
            "epoch 26: mae=1.3209946155548096, fscore=0.7297738270805455, mae/fscore=1.810142246160311\n",
            "model saved.\n",
            "epoch 27: mae=1.3173997402191162, fscore=0.7322351314461748, mae/fscore=1.799148256793426\n",
            "model saved.\n",
            "epoch 28: mae=1.3194812536239624, fscore=0.725651472437657, mae/fscore=1.8183399633400825\n",
            "epoch 29: mae=1.309130072593689, fscore=0.7337740277865888, mae/fscore=1.7841049759312813\n",
            "model saved.\n",
            "epoch 30: mae=1.3072937726974487, fscore=0.7126929524992238, mae/fscore=1.8343012719334353\n",
            "epoch 31: mae=1.3017935752868652, fscore=0.7348071109562968, mae/fscore=1.7716124119041756\n",
            "model saved.\n",
            "epoch 32: mae=1.3064286708831787, fscore=0.7333947524515063, mae/fscore=1.7813442056706592\n",
            "epoch 33: mae=1.3011125326156616, fscore=0.7347260985597002, mae/fscore=1.7708808195029129\n",
            "model saved.\n",
            "epoch 34: mae=1.30738365650177, fscore=0.7344154958395853, mae/fscore=1.7801686999949484\n",
            "epoch 35: mae=1.2945500612258911, fscore=0.7366403590494733, mae/fscore=1.7573702955391814\n",
            "model saved.\n",
            "epoch 36: mae=1.2915173768997192, fscore=0.7309734289436076, mae/fscore=1.7668456185632353\n",
            "epoch 37: mae=1.2882674932479858, fscore=0.7359646899184348, mae/fscore=1.7504471829294463\n",
            "model saved.\n",
            "epoch 38: mae=1.29335355758667, fscore=0.7370666963634891, mae/fscore=1.754730458579557\n",
            "epoch 39: mae=1.286940574645996, fscore=0.740049330173623, mae/fscore=1.7389927242347407\n",
            "model saved.\n",
            "epoch 40: mae=1.2837291955947876, fscore=0.7402315795319439, mae/fscore=1.7342262309098997\n",
            "model saved.\n",
            "epoch 41: mae=1.2803809642791748, fscore=0.7421117045231911, mae/fscore=1.7253208431333649\n",
            "model saved.\n",
            "epoch 42: mae=1.2745829820632935, fscore=0.7353070859900221, mae/fscore=1.7334020479443393\n",
            "epoch 43: mae=1.278261423110962, fscore=0.7417879967080865, mae/fscore=1.7232164128591998\n",
            "model saved.\n",
            "epoch 44: mae=1.2765079736709595, fscore=0.7377166357097193, mae/fscore=1.730349756052229\n",
            "epoch 45: mae=1.27379310131073, fscore=0.7425010151656927, mae/fscore=1.715543687805071\n",
            "model saved.\n",
            "epoch 46: mae=1.2701683044433594, fscore=0.7411090566431238, mae/fscore=1.7138747957137987\n",
            "model saved.\n",
            "epoch 47: mae=1.2675904035568237, fscore=0.743552653540161, mae/fscore=1.7047753471715956\n",
            "model saved.\n",
            "epoch 48: mae=1.2731447219848633, fscore=0.7441817001684561, mae/fscore=1.7107979820208838\n",
            "epoch 49: mae=1.2673907279968262, fscore=0.7454063212247121, mae/fscore=1.7002680576785043\n",
            "model saved.\n",
            "model loaded.\n",
            "final val: mae=1.2760499715805054, fscore=0.7495136526432356, mae/fscore=1.7025037460358625\n",
            "submission uploaded.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
