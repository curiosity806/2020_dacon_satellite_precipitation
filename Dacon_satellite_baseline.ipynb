{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dacon_satellite_baseline",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/curiosity806/2020_dacon_satellite_precipitation/blob/Theo/Dacon_satellite_baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oi-MWY4HOblX",
        "colab_type": "code",
        "outputId": "46f53839-b7ee-49b6-aac4-ddcfa35e80b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6La-zpczGzY",
        "colab_type": "code",
        "outputId": "387a6595-3a1e-44e2-cbdb-9a8f20e2eaf8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import random\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Dropout, Conv2DTranspose, MaxPooling2D, BatchNormalization, Activation, concatenate, Input, GlobalAveragePooling2D\n",
        "from tensorflow.keras import Model\n",
        "import warnings\n",
        "import gc\n",
        " \n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YFPNtasGtc-",
        "colab_type": "text"
      },
      "source": [
        "## 1. Data collection and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJ9oJs3QykgG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1.1 Data Collection\n",
        "# train\n",
        "train_origin = np.load('/content/drive/My Drive/2020 Kaggle Study/data/yeonung/train.npy')[:,:,:,:12]\n",
        "\n",
        "# feature\n",
        "train_temp = train_origin[:,:,:,:9]\n",
        "\n",
        "train_landtype = train_origin[:,:,:,9]\n",
        "train_lon_GMI = train_origin[:,:,:,10]\n",
        "train_lat_GMI = train_origin[:,:,:,11]\n",
        "train_DEM = np.load(\"/content/drive/My Drive/2020 Kaggle Study/data/DEM_DPR/dem_train_DPR.npy\")\n",
        "\n",
        "# target\n",
        "train_rain = np.load(\"/content/drive/My Drive/2020 Kaggle Study/data/gmi_preci_32bits.npy\")\n",
        "\n",
        "# test\n",
        "test_origin = np.load('/content/drive/My Drive/2020 Kaggle Study/data/yeonung/test.npy')[:,:,:,:12]\n",
        "\n",
        "# feature\n",
        "test_temp = test_origin[:,:,:,:9]\n",
        "\n",
        "test_landtype = test_origin[:,:,:,9]\n",
        "test_lon_GMI = test_origin[:,:,:,10]\n",
        "test_lat_GMI = test_origin[:,:,:,11]\n",
        "test_DEM = np.load(\"/content/drive/My Drive/2020 Kaggle Study/data/DEM_DPR/dem_test_DPR.npy\")\n",
        "\n",
        "del train_origin\n",
        "\n",
        "all_temp = np.concatenate((train_temp, test_temp), axis = 0)\n",
        "del train_temp, test_temp\n",
        "all_landtype = np.concatenate((train_landtype, test_landtype), axis = 0)\n",
        "del train_landtype, test_landtype\n",
        "all_lon_GMI = np.concatenate((train_lon_GMI, test_lon_GMI), axis = 0)\n",
        "del train_lon_GMI, test_lon_GMI\n",
        "all_lat_GMI = np.concatenate((train_lat_GMI, test_lat_GMI), axis = 0)\n",
        "del train_lat_GMI, test_lat_GMI\n",
        "all_DEM = np.concatenate((train_DEM, test_DEM), axis = 0)\n",
        "del train_DEM, test_DEM"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zB4-ji5bKFdd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3510f98e-1c84-430a-9580-41f26d67988c"
      },
      "source": [
        "all_temp.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(78761, 40, 40, 9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjwXLazCEid1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1.2 Data preprocessing\n",
        "def minmaxscaler(data_info, n):\n",
        "  from sklearn.preprocessing import MinMaxScaler\n",
        "  scaler = MinMaxScaler()\n",
        "  data_in = data_info.reshape(-1, 1)\n",
        "  scaler.fit(data_in)\n",
        "  data_in = scaler.transform(data_in)\n",
        "  data_in = data_in.reshape(-1)\n",
        "  data_info = data_in.reshape(n, 40, 40, 1)\n",
        "  return data_info\n",
        "\n",
        "def standardscaler(data_info):\n",
        "  from sklearn.preprocessing import StandardScaler\n",
        "  scaler = StandardScaler()\n",
        "  data_in = data_info.reshape(-1, 1)\n",
        "  scaler.fit(data_in)\n",
        "  data_in = scaler.transform(data_in)\n",
        "  data_in = data_in.reshape(-1)\n",
        "  data_info = data_in.reshape(78761, 40, 40, 1)\n",
        "  return data_info"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olbssEV6_AhM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "outputId": "d76f7155-1a66-42e3-e22d-92c6922e7c27"
      },
      "source": [
        "# temp - standard , land - categorization, dem - minmax, rain - original\n",
        "print(\"temp\")\n",
        "for i in range(0,9):\n",
        "  t_temp = standardscaler(all_temp[:,:,:,i])\n",
        "  print(\"cat : %d\" %i)\n",
        "  all_temp[:,:,:,i:i+1] = t_temp\n",
        "\n",
        "print(\"land\")\n",
        "t_land = all_landtype.reshape(-1)\n",
        "for i in range(len(t_land)):\n",
        "  t_land[i] = int(str(t_land[i])[0])\n",
        "all_landtype = t_land.reshape(78761,40, 40, 1)\n",
        "\n",
        "print(\"DEM\")\n",
        "all_DEM = minmaxscaler(all_DEM, 78761)\n",
        "\n",
        "print(\"GMI lon, lat\")\n",
        "all_lon_GMI = standardscaler(all_lon_GMI)\n",
        "all_lat_GMI = standardscaler(all_lat_GMI)\n",
        "\n",
        "#print(\"train rain\")\n",
        "train_rain = np.where(train_rain>0, train_rain, 0)\n",
        "#train_rain = minmaxscaler(train_rain, 76345)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "temp\n",
            "cat : 0\n",
            "cat : 1\n",
            "cat : 2\n",
            "cat : 3\n",
            "cat : 4\n",
            "cat : 5\n",
            "cat : 6\n",
            "cat : 7\n",
            "cat : 8\n",
            "land\n",
            "DEM\n",
            "GMI lon, lat\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8Nm2VFrl5pu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = np.concatenate((all_temp[:76345,:,:,:], all_landtype[:76345,:,:,:], all_DEM[:76345,:,:,:], all_lon_GMI[:76345,:,:,:], all_lat_GMI[:76345,:,:,:], train_rain), axis = 3)\n",
        "test = np.concatenate((all_temp[76345:,:,:,:], all_landtype[76345:,:,:,:], all_DEM[76345:,:,:,:], all_lon_GMI[76345:,:,:,:], all_lat_GMI[76345:,:,:,:]), axis = 3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcXAzdyTNRZl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del all_temp, all_landtype, all_DEM, all_lon_GMI, all_lat_GMI, train_rain"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcaeY00rMd4a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = tf.convert_to_tensor(train, dtype=np.float32)\n",
        "train = train.numpy()\n",
        "test = tf.convert_to_tensor(test, dtype=np.float32)\n",
        "test = test.numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnOFP8UOQ9Ip",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "2f082ec7-4d08-4e2d-c00b-a13385b0114f"
      },
      "source": [
        "print(train.shape)\n",
        "print(test.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(76345, 40, 40, 14)\n",
            "(2416, 40, 40, 13)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjkU3PtCNYy8",
        "colab_type": "text"
      },
      "source": [
        "## 2. 모델 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKjP-Uobze5P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 재생산성을 위해 시드 고정\n",
        "np.random.seed(7)\n",
        "random.seed(7)\n",
        "tf.random.set_seed(7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0wWXJKu09ld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(input_layer_c, input_layer_d, start_neurons):\n",
        "\n",
        "    # 40 x 40 -> 20 x 20\n",
        "    convc1 = Conv2D(start_neurons * 1, (3, 3), activation=tf.keras.layers.LeakyReLU(alpha=0.1), padding=\"same\", kernel_initializer='glorot_uniform')(input_layer_c)\n",
        "    convc1 = Conv2D(start_neurons * 1, (3, 3), activation=tf.keras.layers.LeakyReLU(alpha=0.1), padding=\"same\", kernel_initializer='glorot_uniform')(convc1)\n",
        "    poolc1 = BatchNormalization()(convc1)\n",
        "    poolc1 = MaxPooling2D((2, 2))(poolc1)\n",
        "    poolc1 = Dropout(0.25)(poolc1)\n",
        "    # 20 x 20 -> 10 x 10\n",
        "    convc2 = Conv2D(start_neurons * 2, (3, 3), activation=tf.keras.layers.LeakyReLU(alpha=0.1), padding=\"same\", kernel_initializer='glorot_uniform')(poolc1)\n",
        "    convc2 = Conv2D(start_neurons * 2, (3, 3), activation=tf.keras.layers.LeakyReLU(alpha=0.1), padding=\"same\", kernel_initializer='glorot_uniform')(convc2)\n",
        "    poolc2 = BatchNormalization()(convc2)\n",
        "    poolc2 = MaxPooling2D((2, 2))(poolc2)\n",
        "    poolc2 = Dropout(0.25)(poolc2)\n",
        "    # 10 x 10 -> 5 x 5\n",
        "    convc3 = Conv2D(start_neurons * 4, (3, 3), activation=tf.keras.layers.LeakyReLU(alpha=0.1), padding=\"same\", kernel_initializer='glorot_uniform')(poolc2)\n",
        "    convc3 = Conv2D(start_neurons * 4, (3, 3), activation=tf.keras.layers.LeakyReLU(alpha=0.1), padding=\"same\", kernel_initializer='glorot_uniform')(convc3)\n",
        "    poolc3 = BatchNormalization()(convc3)\n",
        "    poolc3 = MaxPooling2D((2, 2))(poolc3)\n",
        "    poolc3 = Dropout(0.25)(poolc3)\n",
        "\n",
        "    # 5 x 5\n",
        "    convmc = Conv2D(start_neurons * 8, (3, 3), activation=tf.keras.layers.LeakyReLU(alpha=0.1), padding=\"same\", kernel_initializer='glorot_uniform')(poolc3)\n",
        "    convmc = BatchNormalization()(convmc)\n",
        "\n",
        "    # 40 x 40 -> 20 x 20\n",
        "    convd1 = Conv2D(start_neurons * 1, (3, 3), activation=tf.keras.layers.LeakyReLU(alpha=0.1), padding=\"same\", kernel_initializer='glorot_uniform')(input_layer_d)\n",
        "    convd1 = Conv2D(start_neurons * 1, (3, 3), activation=tf.keras.layers.LeakyReLU(alpha=0.1), padding=\"same\", kernel_initializer='glorot_uniform')(convd1)\n",
        "    poold1 = BatchNormalization()(convd1)\n",
        "    poold1 = MaxPooling2D((2, 2))(poold1)\n",
        "    poold1 = Dropout(0.25)(poold1)\n",
        "    # 20 x 20 -> 10 x 10\n",
        "    convd2 = Conv2D(start_neurons * 2, (3, 3), activation=tf.keras.layers.LeakyReLU(alpha=0.1), padding=\"same\", kernel_initializer='glorot_uniform')(poold1)\n",
        "    convd2 = Conv2D(start_neurons * 2, (3, 3), activation=tf.keras.layers.LeakyReLU(alpha=0.1), padding=\"same\", kernel_initializer='glorot_uniform')(convd2)\n",
        "    poold2 = BatchNormalization()(convd2)\n",
        "    poold2 = MaxPooling2D((2, 2))(poold2)\n",
        "    poold2 = Dropout(0.25)(poold2)\n",
        "    # 10 x 10 -> 5 x 5\n",
        "    convd3 = Conv2D(start_neurons * 4, (3, 3), activation=tf.keras.layers.LeakyReLU(alpha=0.1), padding=\"same\", kernel_initializer='glorot_uniform')(poold2)\n",
        "    convd3 = Conv2D(start_neurons * 4, (3, 3), activation=tf.keras.layers.LeakyReLU(alpha=0.1), padding=\"same\", kernel_initializer='glorot_uniform')(convd3)\n",
        "    poold3 = BatchNormalization()(convd3)\n",
        "    poold3 = MaxPooling2D((2, 2))(poold3)\n",
        "    poold3 = Dropout(0.25)(poold3)\n",
        "    # 5 x 5\n",
        "    convmd = Conv2D(start_neurons * 8, (3, 3), activation=tf.keras.layers.LeakyReLU(alpha=0.1), padding=\"same\", kernel_initializer='glorot_uniform')(poold3)\n",
        "    convmd = BatchNormalization()(convmd)\n",
        "\n",
        "    convm = concatenate([convmc, convmd], axis = 3)\n",
        "    convm = BatchNormalization()(convm)\n",
        "\n",
        "    # 5 x 5 -> 10 x 10\n",
        "    deconv3 = Conv2DTranspose(start_neurons * 4, (3, 3), strides=(2, 2), activation=tf.keras.layers.LeakyReLU(alpha=0.1), padding=\"same\", kernel_initializer='glorot_uniform')(convm)\n",
        "    uconv3 = concatenate([deconv3, convc3, convd3])\n",
        "    uconv3 = Dropout(0.25)(uconv3)\n",
        "    uconv3 = Conv2D(start_neurons * 2, (3, 3), activation=tf.keras.layers.LeakyReLU(alpha=0.1), padding=\"same\", kernel_initializer='glorot_uniform')(uconv3)\n",
        "    uconv3 = Conv2D(start_neurons * 2, (3, 3), activation=tf.keras.layers.LeakyReLU(alpha=0.1), padding=\"same\", kernel_initializer='glorot_uniform')(uconv3)\n",
        "    uconv3 = BatchNormalization()(uconv3)\n",
        "    # 10 x 10 -> 20 x 20\n",
        "    deconv2 = Conv2DTranspose(start_neurons * 2, (3, 3), strides=(2, 2), activation=tf.keras.layers.LeakyReLU(alpha=0.1), padding=\"same\", kernel_initializer='glorot_uniform')(uconv3)\n",
        "    uconv2 = concatenate([deconv2, convc2, convd2])\n",
        "    uconv2 = Dropout(0.25)(uconv2)\n",
        "    uconv2 = Conv2D(start_neurons * 2, (3, 3), activation=tf.keras.layers.LeakyReLU(alpha=0.1), padding=\"same\", kernel_initializer='glorot_uniform')(uconv2)\n",
        "    uconv2 = Conv2D(start_neurons * 2, (3, 3), activation=tf.keras.layers.LeakyReLU(alpha=0.1), padding=\"same\", kernel_initializer='glorot_uniform')(uconv2)\n",
        "    uconv2 = BatchNormalization()(uconv2)\n",
        "    # 20 x 20 -> 40 x 40\n",
        "    deconv1 = Conv2DTranspose(start_neurons * 1, (3, 3), strides=(2, 2), padding=\"same\", activation = tf.keras.layers.LeakyReLU(alpha=0.1))(uconv2)\n",
        "    uconv1 = concatenate([deconv1, convc1, convd1])\n",
        "    uconv1 = Dropout(0.25)(uconv1)\n",
        "    uconv1 = Conv2D(start_neurons * 1, (3, 3), activation=tf.keras.layers.LeakyReLU(alpha=0.1), padding=\"same\", kernel_initializer='glorot_uniform')(uconv1)\n",
        "    uconv1 = Conv2D(start_neurons * 1, (3, 3), activation=tf.keras.layers.LeakyReLU(alpha=0.1), padding=\"same\", kernel_initializer='glorot_uniform')(uconv1)\n",
        "    uconv1 = BatchNormalization()(uconv1)\n",
        "    uconv1 = Dropout(0.25)(uconv1)\n",
        "    output_layer = Conv2D(1, (1,1),activation=tf.keras.layers.LeakyReLU(alpha=0.1), padding=\"same\", kernel_initializer='glorot_uniform')(uconv1)\n",
        "    \n",
        "    return output_layer\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXRyvpMVr21K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_layer_c = Input((40, 40, 9))\n",
        "input_layer_d = Input((40, 40, 4))\n",
        "output_layer = build_model(input_layer_c, input_layer_d,  32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqMt4p1N09nx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Model(inputs = [input_layer_c, input_layer_d], outputs = [output_layer])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_NhbS4s09p_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "def mae(y_true, y_pred) :\n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "    y_true = y_true.reshape(1, -1)[0]\n",
        "    y_pred = y_pred.reshape(1, -1)[0]\n",
        "    over_threshold = y_true >= 0.1\n",
        "    return np.mean(np.abs(y_true[over_threshold] - y_pred[over_threshold]))\n",
        "def fscore(y_true, y_pred):\n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "    y_true = y_true.reshape(1, -1)[0]\n",
        "    y_pred = y_pred.reshape(1, -1)[0]\n",
        "    remove_NAs = y_true >= 0\n",
        "    y_true = np.where(y_true[remove_NAs] >= 0.1, 1, 0)\n",
        "    y_pred = np.where(y_pred[remove_NAs] >= 0.1, 1, 0)\n",
        "    return(f1_score(y_true, y_pred))\n",
        "def maeOverFscore(y_true, y_pred):\n",
        "    return mae(y_true, y_pred) / (fscore(y_true, y_pred) + 1e-07)\n",
        "def fscore_keras(y_true, y_pred):\n",
        "    score = tf.py_function(func=fscore, inp=[y_true, y_pred], Tout=tf.float32, name='fscore_keras')\n",
        "    return score\n",
        "def maeOverFscore_keras(y_true, y_pred):\n",
        "    score = tf.py_function(func=maeOverFscore, inp=[y_true, y_pred], Tout=tf.float32,  name='custom_mse') \n",
        "    return score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sv7Hk_LM1DNf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(loss=\"mae\", optimizer=opt, metrics=[maeOverFscore_keras, fscore_keras])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zxke2xER1EHj",
        "colab_type": "code",
        "outputId": "690b6ad6-66b3-4c0e-c867-8eca1f132b36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        }
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "kf = KFold(n_splits=5, shuffle = True)\n",
        "model_history = []\n",
        "fold_number = 0\n",
        "\n",
        "for train_index, test_index in kf.split(train):\n",
        "    print(f'\\nFold {fold_number}')\n",
        "    fold_number = fold_number + 1\n",
        "    \n",
        "#-----------------------------------------------------------------------------\n",
        "    def trainGenerator():\n",
        "        print(1)\n",
        "        for file in train[train_index]:\n",
        "            target= file[:,:,-1:].reshape(40,40,1)\n",
        "            cutoff_labels = np.where(target < 0, 0, target)\n",
        "            feature_c = file[:,:,:9]\n",
        "            feature_d = file[:,:,9:13]\n",
        "            if (cutoff_labels > 0).sum() < 50:\n",
        "                continue\n",
        "            yield (feature_c, feature_d), cutoff_labels\n",
        "\n",
        "    train_dataset = tf.data.Dataset.from_generator(trainGenerator,\n",
        "                                                ((tf.float32, tf.float32), tf.float32),\n",
        "                                                ((tf.TensorShape([40,40,9]), tf.TensorShape([40, 40, 4])), tf.TensorShape([40,40,1])))\n",
        "    \n",
        "    train_dataset = train_dataset.batch(512).prefetch(1)\n",
        "#-----------------------------------------------------------------------------\n",
        "    def testGenerator():\n",
        "        print(2)\n",
        "        for file in train[test_index]:\n",
        "            target= file[:,:,-1:].reshape(40,40,1)\n",
        "            cutoff_labels = np.where(target < 0, 0, target)\n",
        "            feature_c = file[:,:,:9]\n",
        "            feature_d = file[:,:,9:13]\n",
        "            if (cutoff_labels > 0).sum() < 50:\n",
        "                continue\n",
        "            yield (feature_c, feature_d), cutoff_labels\n",
        "    test_dataset = tf.data.Dataset.from_generator(testGenerator,\n",
        "                                                ((tf.float32, tf.float32), tf.float32),\n",
        "                                                ((tf.TensorShape([40,40,9]), tf.TensorShape([40, 40, 4])),tf.TensorShape([40,40,1])))\n",
        "    test_size = len(test_index)\n",
        "    test_dataset = test_dataset.batch(test_size).prefetch(1)\n",
        "#-----------------------------------------------------------------------------\n",
        "    history = model.fit(train_dataset, epochs = 20, verbose=1, validation_data = test_dataset)\n",
        "    model.save(\"/content/drive/My Drive/2020 Kaggle Study/Theos/Model/three_leg_CNN_attention.h5\")\n",
        "    #history = model.fit(train_dataset, epochs = 5, ,batch_size = 11, verbose=1)\n",
        "    model_history.append(history)\n",
        "    #del data\n",
        "    del train_dataset\n",
        "    del test_dataset\n",
        "    gc.collect()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Fold 0\n",
            "Epoch 1/20\n",
            "1\n",
            "     77/Unknown - 241s 3s/step - loss: 0.2407 - maeOverFscore_keras: 2.8778 - fscore_keras: 0.54252\n",
            "77/77 [==============================] - 271s 4s/step - loss: 0.2407 - maeOverFscore_keras: 2.8778 - fscore_keras: 0.5425 - val_loss: 0.1814 - val_maeOverFscore_keras: 2.1954 - val_fscore_keras: 0.6243\n",
            "Epoch 2/20\n",
            "1\n",
            "77/77 [==============================] - ETA: 0s - loss: 0.1757 - maeOverFscore_keras: 1.8922 - fscore_keras: 0.66372\n",
            "77/77 [==============================] - 280s 4s/step - loss: 0.1757 - maeOverFscore_keras: 1.8922 - fscore_keras: 0.6637 - val_loss: 0.2009 - val_maeOverFscore_keras: 1.8081 - val_fscore_keras: 0.6818\n",
            "Epoch 3/20\n",
            "1\n",
            "77/77 [==============================] - ETA: 0s - loss: 0.1654 - maeOverFscore_keras: 1.7240 - fscore_keras: 0.69392\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kucgrytn8-yj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.models.load_model('/content/drive/My Drive/2020 Kaggle Study/Theos/Model/three_leg_CNN_5.h5', custom_objects={\"maeOverFscore_keras\": maeOverFscore_keras,\"LeakyReLU\":LeakyReLU, \"fscore_keras\":fscore_keras })"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFIVaK6rSRZs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#test_tempandland = np.load(\"/content/drive/My Drive/2020 Kaggle Study/data/yeonung/test.npy\")[:,:,:,:10]\n",
        "#test_DEM = np.load(\"/content/drive/My Drive/2020 Kaggle Study/data/DEM_DPR/dem_test_DPR.npy\")\n",
        "#test_day = np.load(\"/content/drive/My Drive/2020 Kaggle Study/data/orbit_no_shuffle/test_days.npy\")\n",
        "#test_times = np.load(\"/content/drive/My Drive/2020 Kaggle Study/data/orbit_no_shuffle/test_times.npy\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdTi7FZBVEKC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#test = np.concatenate((test_tempandland, test_DEM, test_day, test_times), axis = 3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raJnv0_aTVCH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "519b358c-8692-4adb-d5d9-81a74b61997e"
      },
      "source": [
        "print(test.shape)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2416, 40, 40, 13)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZ1bQvRSVNLl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred=model.predict([test[:,:,:,0:9], test[:,:,:,9:]])\n",
        "pred = np.where(pred>0, pred, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vo6hXSOVQkj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submission = pd.read_csv('/content/drive/My Drive/2020 Kaggle Study/data/sample_submission.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUTWSL8LVn79",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submission.iloc[:,1:] = pred.reshape(-1, 1600)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvlSMMFXVp38",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submission.to_csv('/content/drive/My Drive/2020 Kaggle Study/Theos/Submission/three_leg_CNN_second.csv', index = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_C5YSDK-8orZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}