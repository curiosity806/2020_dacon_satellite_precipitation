{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dacon_satellite_baseline.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/curiosity806/2020_dacon_satellite_precipitation/blob/rotation-flip%2Fbaseline/Dacon_satellite_baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsaIvLKHsjva",
        "colab_type": "text"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oi-MWY4HOblX",
        "colab_type": "code",
        "outputId": "93f846e4-ad92-4879-c906-b9a628700dd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6La-zpczGzY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, Dropout, MaxPooling2D, BatchNormalization, concatenate, Input\n",
        "from tensorflow.keras import Model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKjP-Uobze5P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 재생산성을 위해 시드 고정\n",
        "np.random.seed(7)\n",
        "random.seed(7)\n",
        "tf.random.set_seed(7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pUOc4bq-IrZ",
        "colab_type": "text"
      },
      "source": [
        "## 데이터 받아오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1iW93kj0O2u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os.path\n",
        "\n",
        "if not os.path.isfile('train.npy'):\n",
        "    !cp '/content/drive/My Drive/2020 Kaggle Study/data/train.npy' train.npy\n",
        "if not os.path.isfile('test.npy'): \n",
        "    !cp '/content/drive/My Drive/2020 Kaggle Study/data/test.npy' test.npy\n",
        "if not os.path.isfile('gmi_preci.npy'):\n",
        "    !cp '/content/drive/My Drive/2020 Kaggle Study/data/gmi_preci.npy' gmi_preci.npy\n",
        "\n",
        "train = np.load('train.npy')  # float32\n",
        "test = np.load('test.npy')  # float64\n",
        "gmi_preci = np.load('gmi_preci.npy')  # float32\n",
        "train[:, :, :, -1] = gmi_preci.reshape(-1, 40, 40)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-S9OXyd9K-a",
        "colab_type": "text"
      },
      "source": [
        "## Train test split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsowKWD39Qsi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train, val = train_test_split(train, test_size=0.025, random_state=7777)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbBPp8ebseqp",
        "colab_type": "text"
      },
      "source": [
        "## Data preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3Gcx_uZv8xW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(train[:,:,:,:9].reshape(-1, 9))\n",
        "\n",
        "def preprocess(data):\n",
        "    # Normalize sensor data\n",
        "    data[:,:,:,:9] = scaler.transform(data[:,:,:,:9].reshape(-1, 9)).reshape(-1, 40, 40, 9)\n",
        "\n",
        "    # Land type\n",
        "    land_type_data = data[:,:,:,9]\n",
        "    data[:,:,:,9] = np.where(land_type_data//100 == 2, 0.8,\n",
        "                             np.where(land_type_data//100 == 3, 0.1,\n",
        "                                      land_type_data//100))\n",
        "\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCpO8Eenso9k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -9999를 포함한 이미지, 강수인 지역이 50픽셀 미만인 이미지 제거\n",
        "is_valid = (train[:,:,:,-1].reshape(-1, 1600) < 0).sum(axis=1) == 0\n",
        "is_valid = is_valid & ((train[:,:,:,-1].reshape(-1, 1600) >= 0.1).sum(axis=1) >= 50)\n",
        "train = train[is_valid]  # (-1, 40, 40, 15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGj-ULPdRw4f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = preprocess(train)\n",
        "val = preprocess(val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUGkDFCTqVSw",
        "colab_type": "text"
      },
      "source": [
        "## Rotation-flip augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5WMJFagqZpc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def r(a, n=1):\n",
        "    return np.rot90(a, n, (1, 2))\n",
        "\n",
        "def f_ud(a):\n",
        "    return np.flip(a, 1)\n",
        "\n",
        "def f_lr(a):\n",
        "    return np.flip(a, 2)\n",
        "\n",
        "def t(a):\n",
        "    return np.transpose(a, (0, 2, 1, 3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zte0DnFAqbWr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "abcd = train\n",
        "acbd = r(f_lr(train))\n",
        "badc = f_lr(train)\n",
        "bdac = r(train)\n",
        "cadb = r(train, 3)\n",
        "cdab = f_ud(train)\n",
        "dbca = t(r(train, 2))\n",
        "dcba = r(train, 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pYZYG3G-Itsk"
      },
      "source": [
        "## 모델만들기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_NhbS4s09p_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def mae(y_true, y_pred) :\n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "    y_true = y_true.reshape(1, -1)[0]\n",
        "    y_pred = y_pred.reshape(1, -1)[0]\n",
        "    over_threshold = y_true >= 0.1\n",
        "    return np.mean(np.abs(y_true[over_threshold] - y_pred[over_threshold]))\n",
        "\n",
        "def fscore(y_true, y_pred):\n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "    y_true = y_true.reshape(1, -1)[0]\n",
        "    y_pred = y_pred.reshape(1, -1)[0]\n",
        "    remove_NAs = y_true >= 0\n",
        "    y_true = np.where(y_true[remove_NAs] >= 0.1, 1, 0)\n",
        "    y_pred = np.where(y_pred[remove_NAs] >= 0.1, 1, 0)\n",
        "    return(f1_score(y_true, y_pred))\n",
        "\n",
        "def maeOverFscore(y_true, y_pred):\n",
        "    return mae(y_true, y_pred) / (fscore(y_true, y_pred) + 1e-07)\n",
        "\n",
        "def fscore_keras(y_true, y_pred):\n",
        "    score = tf.py_function(func=fscore, inp=[y_true, y_pred], Tout=tf.float32, name='fscore_keras')\n",
        "    return score\n",
        "\n",
        "def maeOverFscore_keras(y_true, y_pred):\n",
        "    score = tf.py_function(func=maeOverFscore, inp=[y_true, y_pred], Tout=tf.float32,  name='custom_mse') \n",
        "    return score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vBZ09E8JZpC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(start_neurons=32):\n",
        "    # 40 x 40 -> 20 x 20\n",
        "    input_layer = Input((40, 40, 10))\n",
        "    conv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(input_layer)\n",
        "    conv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(conv1)\n",
        "    pool1 = BatchNormalization()(conv1)\n",
        "    pool1 = MaxPooling2D((2, 2))(pool1)\n",
        "    pool1 = Dropout(0.25)(pool1)\n",
        "\n",
        "    # 20 x 20 -> 10 x 10\n",
        "    conv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(pool1)\n",
        "    conv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(conv2)\n",
        "    pool2 = BatchNormalization()(conv2)\n",
        "    pool2 = MaxPooling2D((2, 2))(pool2)\n",
        "    pool2 = Dropout(0.25)(pool2)\n",
        "\n",
        "    # 10 x 10 \n",
        "    convm = Conv2D(start_neurons * 4, (3, 3), activation=\"relu\", padding=\"same\")(pool2)\n",
        "\n",
        "    # 10 x 10 -> 20 x 20\n",
        "    deconv2 = Conv2DTranspose(start_neurons * 2, (3, 3), strides=(2, 2), padding=\"same\")(convm)\n",
        "    uconv2 = concatenate([deconv2, conv2])\n",
        "    uconv2 = Dropout(0.25)(uconv2)\n",
        "    uconv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(uconv2)\n",
        "    uconv2 = Conv2D(start_neurons * 2, (3, 3), activation=\"relu\", padding=\"same\")(uconv2)\n",
        "    uconv2 = BatchNormalization()(uconv2)\n",
        "\n",
        "    # 20 x 20 -> 40 x 40\n",
        "    deconv1 = Conv2DTranspose(start_neurons * 1, (3, 3), strides=(2, 2), padding=\"same\")(uconv2)\n",
        "    uconv1 = concatenate([deconv1, conv1])\n",
        "    uconv1 = Dropout(0.25)(uconv1)\n",
        "    uconv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(uconv1)\n",
        "    uconv1 = Conv2D(start_neurons * 1, (3, 3), activation=\"relu\", padding=\"same\")(uconv1)\n",
        "    uconv1 = BatchNormalization()(uconv1)\n",
        "    uconv1 = Dropout(0.25)(uconv1)\n",
        "    output_layer = Conv2D(1, (1, 1), padding=\"same\", activation='relu')(uconv1)\n",
        "\n",
        "    return Model(input_layer, output_layer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nWrybUnXIts7"
      },
      "source": [
        "## 모델 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brkWZJad6f8N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d469f706-2f5c-451f-a7fc-08cf0767ff76"
      },
      "source": [
        "import gc\n",
        "\n",
        "x_val, y_val = val[:, :, :, :10], val[:, :, :, -1]\n",
        "train_data = [[abcd, acbd], [badc, bdac], [cadb, cdab], [dbca, dcba]]\n",
        "\n",
        "models = []\n",
        "for t in train_data:\n",
        "    _train = np.concatenate(t)\n",
        "    x_train, y_train = _train[:, :, :, :10], _train[:, :, :, -1]\n",
        "\n",
        "    model = build_model()\n",
        "    model.compile(loss=\"mae\", optimizer=\"adam\", metrics=[maeOverFscore_keras, fscore_keras])\n",
        "    model.fit(x_train, y_train, epochs=15, batch_size=512, validation_data=(x_val, y_val))\n",
        "    models.append(model)\n",
        "\n",
        "    del x_train, y_train\n",
        "    del _train\n",
        "    gc.collect()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "131/131 [==============================] - 92s 699ms/step - loss: 0.2651 - maeOverFscore_keras: 2.5795 - fscore_keras: 0.6088 - val_loss: 65.8745 - val_maeOverFscore_keras: 2.5883 - val_fscore_keras: 0.5624\n",
            "Epoch 2/15\n",
            "131/131 [==============================] - 91s 692ms/step - loss: 0.2170 - maeOverFscore_keras: 1.8240 - fscore_keras: 0.7078 - val_loss: 65.8446 - val_maeOverFscore_keras: 2.0129 - val_fscore_keras: 0.6339\n",
            "Epoch 3/15\n",
            "131/131 [==============================] - 90s 691ms/step - loss: 0.2088 - maeOverFscore_keras: 1.7047 - fscore_keras: 0.7278 - val_loss: 65.8430 - val_maeOverFscore_keras: 2.0201 - val_fscore_keras: 0.6261\n",
            "Epoch 4/15\n",
            "131/131 [==============================] - 91s 692ms/step - loss: 0.2041 - maeOverFscore_keras: 1.6428 - fscore_keras: 0.7374 - val_loss: 65.8363 - val_maeOverFscore_keras: 1.7813 - val_fscore_keras: 0.6583\n",
            "Epoch 5/15\n",
            "131/131 [==============================] - 90s 690ms/step - loss: 0.2017 - maeOverFscore_keras: 1.6106 - fscore_keras: 0.7434 - val_loss: 65.8317 - val_maeOverFscore_keras: 1.5975 - val_fscore_keras: 0.6988\n",
            "Epoch 6/15\n",
            "131/131 [==============================] - 91s 692ms/step - loss: 0.2000 - maeOverFscore_keras: 1.5844 - fscore_keras: 0.7488 - val_loss: 65.8315 - val_maeOverFscore_keras: 1.5426 - val_fscore_keras: 0.7082\n",
            "Epoch 7/15\n",
            "131/131 [==============================] - 91s 694ms/step - loss: 0.1979 - maeOverFscore_keras: 1.5588 - fscore_keras: 0.7526 - val_loss: 65.8378 - val_maeOverFscore_keras: 1.6932 - val_fscore_keras: 0.6763\n",
            "Epoch 8/15\n",
            "131/131 [==============================] - 91s 693ms/step - loss: 0.1958 - maeOverFscore_keras: 1.5295 - fscore_keras: 0.7591 - val_loss: 65.8315 - val_maeOverFscore_keras: 1.5302 - val_fscore_keras: 0.7148\n",
            "Epoch 9/15\n",
            "131/131 [==============================] - 91s 695ms/step - loss: 0.1948 - maeOverFscore_keras: 1.5184 - fscore_keras: 0.7604 - val_loss: 65.8316 - val_maeOverFscore_keras: 1.5835 - val_fscore_keras: 0.7015\n",
            "Epoch 10/15\n",
            "131/131 [==============================] - 91s 697ms/step - loss: 0.1937 - maeOverFscore_keras: 1.5041 - fscore_keras: 0.7631 - val_loss: 65.8306 - val_maeOverFscore_keras: 1.5263 - val_fscore_keras: 0.7076\n",
            "Epoch 11/15\n",
            "131/131 [==============================] - 91s 692ms/step - loss: 0.1927 - maeOverFscore_keras: 1.4895 - fscore_keras: 0.7663 - val_loss: 65.8340 - val_maeOverFscore_keras: 1.4485 - val_fscore_keras: 0.7178\n",
            "Epoch 12/15\n",
            "131/131 [==============================] - 91s 694ms/step - loss: 0.1909 - maeOverFscore_keras: 1.4715 - fscore_keras: 0.7686 - val_loss: 65.8281 - val_maeOverFscore_keras: 1.4225 - val_fscore_keras: 0.7339\n",
            "Epoch 13/15\n",
            "131/131 [==============================] - 90s 691ms/step - loss: 0.1901 - maeOverFscore_keras: 1.4624 - fscore_keras: 0.7698 - val_loss: 65.8291 - val_maeOverFscore_keras: 1.4022 - val_fscore_keras: 0.7368\n",
            "Epoch 14/15\n",
            "131/131 [==============================] - 91s 694ms/step - loss: 0.1900 - maeOverFscore_keras: 1.4600 - fscore_keras: 0.7712 - val_loss: 65.8254 - val_maeOverFscore_keras: 1.3760 - val_fscore_keras: 0.7481\n",
            "Epoch 15/15\n",
            "131/131 [==============================] - 92s 702ms/step - loss: 0.1887 - maeOverFscore_keras: 1.4458 - fscore_keras: 0.7729 - val_loss: 65.8290 - val_maeOverFscore_keras: 1.3904 - val_fscore_keras: 0.7462\n",
            "Epoch 1/15\n",
            "131/131 [==============================] - 91s 692ms/step - loss: 0.2576 - maeOverFscore_keras: 2.5927 - fscore_keras: 0.6118 - val_loss: 65.8707 - val_maeOverFscore_keras: 2.4855 - val_fscore_keras: 0.5768\n",
            "Epoch 2/15\n",
            "131/131 [==============================] - 91s 696ms/step - loss: 0.2153 - maeOverFscore_keras: 1.7887 - fscore_keras: 0.7153 - val_loss: 65.8491 - val_maeOverFscore_keras: 1.9876 - val_fscore_keras: 0.6366\n",
            "Epoch 3/15\n",
            "131/131 [==============================] - 91s 696ms/step - loss: 0.2060 - maeOverFscore_keras: 1.6621 - fscore_keras: 0.7362 - val_loss: 65.8407 - val_maeOverFscore_keras: 1.7694 - val_fscore_keras: 0.6829\n",
            "Epoch 4/15\n",
            "131/131 [==============================] - 92s 699ms/step - loss: 0.2014 - maeOverFscore_keras: 1.6009 - fscore_keras: 0.7458 - val_loss: 65.8327 - val_maeOverFscore_keras: 1.5929 - val_fscore_keras: 0.6981\n",
            "Epoch 5/15\n",
            "131/131 [==============================] - 93s 709ms/step - loss: 0.1992 - maeOverFscore_keras: 1.5748 - fscore_keras: 0.7502 - val_loss: 65.8298 - val_maeOverFscore_keras: 1.4676 - val_fscore_keras: 0.7334\n",
            "Epoch 6/15\n",
            "131/131 [==============================] - 92s 701ms/step - loss: 0.1984 - maeOverFscore_keras: 1.5667 - fscore_keras: 0.7504 - val_loss: 65.8301 - val_maeOverFscore_keras: 1.5162 - val_fscore_keras: 0.7164\n",
            "Epoch 7/15\n",
            "131/131 [==============================] - 92s 699ms/step - loss: 0.1952 - maeOverFscore_keras: 1.5203 - fscore_keras: 0.7604 - val_loss: 65.8359 - val_maeOverFscore_keras: 1.6009 - val_fscore_keras: 0.6881\n",
            "Epoch 8/15\n",
            "131/131 [==============================] - 92s 703ms/step - loss: 0.1940 - maeOverFscore_keras: 1.5075 - fscore_keras: 0.7624 - val_loss: 65.8330 - val_maeOverFscore_keras: 1.4745 - val_fscore_keras: 0.7184\n",
            "Epoch 9/15\n",
            "131/131 [==============================] - 92s 701ms/step - loss: 0.1925 - maeOverFscore_keras: 1.4923 - fscore_keras: 0.7642 - val_loss: 65.8315 - val_maeOverFscore_keras: 1.5226 - val_fscore_keras: 0.7162\n",
            "Epoch 10/15\n",
            "131/131 [==============================] - 93s 709ms/step - loss: 0.1922 - maeOverFscore_keras: 1.4851 - fscore_keras: 0.7660 - val_loss: 65.8301 - val_maeOverFscore_keras: 1.4299 - val_fscore_keras: 0.7343\n",
            "Epoch 11/15\n",
            "131/131 [==============================] - 93s 706ms/step - loss: 0.1914 - maeOverFscore_keras: 1.4754 - fscore_keras: 0.7679 - val_loss: 65.8281 - val_maeOverFscore_keras: 1.3834 - val_fscore_keras: 0.7457\n",
            "Epoch 12/15\n",
            "131/131 [==============================] - 92s 706ms/step - loss: 0.1898 - maeOverFscore_keras: 1.4559 - fscore_keras: 0.7712 - val_loss: 65.8264 - val_maeOverFscore_keras: 1.3812 - val_fscore_keras: 0.7472\n",
            "Epoch 13/15\n",
            "131/131 [==============================] - 92s 702ms/step - loss: 0.1893 - maeOverFscore_keras: 1.4495 - fscore_keras: 0.7725 - val_loss: 65.8269 - val_maeOverFscore_keras: 1.3994 - val_fscore_keras: 0.7512\n",
            "Epoch 14/15\n",
            "131/131 [==============================] - 92s 703ms/step - loss: 0.1894 - maeOverFscore_keras: 1.4528 - fscore_keras: 0.7717 - val_loss: 65.8273 - val_maeOverFscore_keras: 1.3685 - val_fscore_keras: 0.7549\n",
            "Epoch 15/15\n",
            "131/131 [==============================] - 92s 705ms/step - loss: 0.1879 - maeOverFscore_keras: 1.4340 - fscore_keras: 0.7749 - val_loss: 65.8269 - val_maeOverFscore_keras: 1.3641 - val_fscore_keras: 0.7597\n",
            "Epoch 1/15\n",
            "131/131 [==============================] - 92s 701ms/step - loss: 0.2606 - maeOverFscore_keras: 2.6663 - fscore_keras: 0.6047 - val_loss: 65.8596 - val_maeOverFscore_keras: 2.2842 - val_fscore_keras: 0.6352\n",
            "Epoch 2/15\n",
            "131/131 [==============================] - 92s 703ms/step - loss: 0.2145 - maeOverFscore_keras: 1.7735 - fscore_keras: 0.7196 - val_loss: 65.8532 - val_maeOverFscore_keras: 2.0449 - val_fscore_keras: 0.6246\n",
            "Epoch 3/15\n",
            "131/131 [==============================] - 91s 698ms/step - loss: 0.2068 - maeOverFscore_keras: 1.6731 - fscore_keras: 0.7351 - val_loss: 65.8402 - val_maeOverFscore_keras: 1.6556 - val_fscore_keras: 0.7125\n",
            "Epoch 4/15\n",
            "127/131 [============================>.] - ETA: 2s - loss: 0.2021 - maeOverFscore_keras: 1.6116 - fscore_keras: 0.7448"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-d39d744fd256>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mae\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"adam\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmaeOverFscore_keras\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscore_keras\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoowuSxyZgLH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "aff8f371-5abf-4577-b449-17d5395a6731"
      },
      "source": [
        "## 모델 저장하기\n",
        "cnt = 0\n",
        "for model in models:\n",
        "    cnt += 1\n",
        "    model_name = f'model{cnt}.h5'\n",
        "    model.save(model_name)\n",
        "    !cp model_name '/content/drive/My Drive/2020 Kaggle Study/model'"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cp: cannot stat 'model_name': No such file or directory\n",
            "cp: cannot stat 'model_name': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjmLe8t7j8cN",
        "colab_type": "text"
      },
      "source": [
        "## Precipatation GMI -> DPR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyTSFuEGkEd4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dr = [(-1, -1), (-1, 0), (-1, 1),\n",
        "      (0, -1), (0, 0), (0, 1),\n",
        "      (1, -1), (1, 0), (1, 1)]\n",
        "\n",
        "# p1, p2: shape=(-1, 2).\n",
        "def get_dist(p1, p2):\n",
        "    x1 = np.deg2rad(p1[:,0])\n",
        "    y1 = np.deg2rad(p1[:,1])\n",
        "    x2 = np.deg2rad(p2[:,0])\n",
        "    y2 = np.deg2rad(p2[:,1])\n",
        "    dlon = x2 - x1\n",
        "    dlat = y2 - y1\n",
        "    a = np.sin(dlat/2)**2 + np.cos(y1) * np.cos(y2) * np.sin(dlon/2)**2 \n",
        "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))    \n",
        "    return 6373.0 * c  # km, shape=(-1).\n",
        "\n",
        "# ori_ll에서의 value 값을 tgt_ll에 대한 값으로 바꾼다.\n",
        "# value: (40, 40, -1)\n",
        "# ori_ll, tgt_ll: (40, 40, 2)\n",
        "# ori_ll: value에 대응되는 원래 latitude, longitude.\n",
        "# tgt_ll: 변환되는 값에 대응되는 latitude, longitude.\n",
        "def compen_ll(value, ori_ll, tgt_ll):\n",
        "    ret = np.empty_like(value)\n",
        "\n",
        "    n = value.shape[0]\n",
        "    m = value.shape[1]\n",
        "\n",
        "    for i in range(n):\n",
        "        for j in range(m):\n",
        "            nears = []  # (row, col, value)\n",
        "            for k in range(9):\n",
        "                ii = i + dr[k][0]\n",
        "                jj = j + dr[k][1]\n",
        "                if ii >= 0 and ii < n and jj >= 0 and jj < m:\n",
        "                    nears.append((ori_ll[ii, jj][0], ori_ll[ii, jj][1],\n",
        "                                  tgt_ll[i, j][0], tgt_ll[i, j][1],\n",
        "                                  value[ii, jj]))\n",
        "\n",
        "            nears = np.array(nears)  # shape=(-1, 5)\n",
        "            dists = get_dist(nears[:, 0:2], nears[:, 2:4]).reshape(-1, 1)\n",
        "            values = nears[:, 4].reshape(-1, 1)\n",
        "            nears = np.concatenate((dists, values), 1)\n",
        "            nears = nears[np.argsort(nears[:, 0])]  # sort by dist\n",
        "            nears = nears[:4, :]  # 가까운 점 4개만 고려\n",
        "\n",
        "            weights = 1 / (nears[:, 0] ** 2 + sys.float_info.epsilon)\n",
        "            weighted_sum = (weights * nears[:, 1]).sum()\n",
        "            ret[i, j] = weighted_sum / weights.sum()\n",
        "    return ret"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSTjD9TqqRXm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from multiprocessing import Process, Manager\n",
        "\n",
        "def proc_func(splitted, dpr_preci, proc_id):\n",
        "    part = splitted[proc_id]\n",
        "    arr = np.empty_like(part[:, :, :, 14])  # shape=(-1, 40, 40)\n",
        "    for i in range(part.shape[0]):\n",
        "        arr[i, :, :] = compen_ll(part[i, :, :, 14], part[i, :, :, 10:12], part[i, :, :, 12:14])\n",
        "        if (i + 1) % 1000 == 0:\n",
        "            print(proc_id, i + 1)\n",
        "    dpr_preci[proc_id] = arr\n",
        "\n",
        "def gmi2dpr(test, gmi_preci):\n",
        "    gmi_preci = gmi_preci.reshape(-1, 40, 40, 1)\n",
        "    data = np.concatenate((test, gmi_preci), axis=3)\n",
        "\n",
        "    n_procs = 4\n",
        "    procs = []\n",
        "    manager = Manager()\n",
        "    dpr_preci = manager.list([None] * n_procs)\n",
        "\n",
        "    # split data into n_procs arrays\n",
        "    n_imgs = data.shape[0]\n",
        "    splitted = np.split(data, np.arange((n_imgs + n_procs - 1) // n_procs, n_imgs, n_imgs // n_procs))\n",
        "\n",
        "    for proc_id in range(n_procs):\n",
        "        proc = Process(target=proc_func, args=(splitted, dpr_preci, proc_id))\n",
        "        proc.start()\n",
        "        procs.append(proc)\n",
        "\n",
        "    for proc in procs:\n",
        "        proc.join()\n",
        "\n",
        "    dpr_preci = np.concatenate(dpr_preci)\n",
        "    return dpr_preci"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gG9A32a6qyyc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = preprocess(test)\n",
        "x_test = test[:, :, :, :10]\n",
        "\n",
        "preds = []\n",
        "for model in models:\n",
        "    pred = model.predict(x_test)\n",
        "    pred = gmi2dpr(test, pred)\n",
        "    preds.append(pred)\n",
        "\n",
        "final_pred = sum(preds) / len(preds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2xM7q7FpIttP"
      },
      "source": [
        "## submission 만들기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BaAuaIqhh_z3",
        "colab": {}
      },
      "source": [
        "submission = pd.read_csv('/content/drive/My Drive/2020 Kaggle Study/data/sample_submission.csv')\n",
        "submission.iloc[:,1:] = final_pred.reshape(-1, 1600)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9fBTzEKnh_0B",
        "colab": {}
      },
      "source": [
        "# 제출 파일 저장하기\n",
        "submission.to_csv('submission1922.csv', index=False)\n",
        "!cp submission1922.csv '/content/drive/My Drive/2020 Kaggle Study/submission'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhagW0E9Y5MP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "outputId": "6947410f-4a8e-4215-923d-bb98762c2316"
      },
      "source": [
        "submission"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>px_1</th>\n",
              "      <th>px_2</th>\n",
              "      <th>px_3</th>\n",
              "      <th>px_4</th>\n",
              "      <th>px_5</th>\n",
              "      <th>px_6</th>\n",
              "      <th>px_7</th>\n",
              "      <th>px_8</th>\n",
              "      <th>px_9</th>\n",
              "      <th>px_10</th>\n",
              "      <th>px_11</th>\n",
              "      <th>px_12</th>\n",
              "      <th>px_13</th>\n",
              "      <th>px_14</th>\n",
              "      <th>px_15</th>\n",
              "      <th>px_16</th>\n",
              "      <th>px_17</th>\n",
              "      <th>px_18</th>\n",
              "      <th>px_19</th>\n",
              "      <th>px_20</th>\n",
              "      <th>px_21</th>\n",
              "      <th>px_22</th>\n",
              "      <th>px_23</th>\n",
              "      <th>px_24</th>\n",
              "      <th>px_25</th>\n",
              "      <th>px_26</th>\n",
              "      <th>px_27</th>\n",
              "      <th>px_28</th>\n",
              "      <th>px_29</th>\n",
              "      <th>px_30</th>\n",
              "      <th>px_31</th>\n",
              "      <th>px_32</th>\n",
              "      <th>px_33</th>\n",
              "      <th>px_34</th>\n",
              "      <th>px_35</th>\n",
              "      <th>px_36</th>\n",
              "      <th>px_37</th>\n",
              "      <th>px_38</th>\n",
              "      <th>px_39</th>\n",
              "      <th>...</th>\n",
              "      <th>px_1561</th>\n",
              "      <th>px_1562</th>\n",
              "      <th>px_1563</th>\n",
              "      <th>px_1564</th>\n",
              "      <th>px_1565</th>\n",
              "      <th>px_1566</th>\n",
              "      <th>px_1567</th>\n",
              "      <th>px_1568</th>\n",
              "      <th>px_1569</th>\n",
              "      <th>px_1570</th>\n",
              "      <th>px_1571</th>\n",
              "      <th>px_1572</th>\n",
              "      <th>px_1573</th>\n",
              "      <th>px_1574</th>\n",
              "      <th>px_1575</th>\n",
              "      <th>px_1576</th>\n",
              "      <th>px_1577</th>\n",
              "      <th>px_1578</th>\n",
              "      <th>px_1579</th>\n",
              "      <th>px_1580</th>\n",
              "      <th>px_1581</th>\n",
              "      <th>px_1582</th>\n",
              "      <th>px_1583</th>\n",
              "      <th>px_1584</th>\n",
              "      <th>px_1585</th>\n",
              "      <th>px_1586</th>\n",
              "      <th>px_1587</th>\n",
              "      <th>px_1588</th>\n",
              "      <th>px_1589</th>\n",
              "      <th>px_1590</th>\n",
              "      <th>px_1591</th>\n",
              "      <th>px_1592</th>\n",
              "      <th>px_1593</th>\n",
              "      <th>px_1594</th>\n",
              "      <th>px_1595</th>\n",
              "      <th>px_1596</th>\n",
              "      <th>px_1597</th>\n",
              "      <th>px_1598</th>\n",
              "      <th>px_1599</th>\n",
              "      <th>px_1600</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>029858_01</td>\n",
              "      <td>0.055712</td>\n",
              "      <td>0.054163</td>\n",
              "      <td>0.010165</td>\n",
              "      <td>0.004527</td>\n",
              "      <td>0.034253</td>\n",
              "      <td>0.093394</td>\n",
              "      <td>0.170403</td>\n",
              "      <td>0.132112</td>\n",
              "      <td>0.054785</td>\n",
              "      <td>0.016913</td>\n",
              "      <td>0.083903</td>\n",
              "      <td>0.340427</td>\n",
              "      <td>0.143531</td>\n",
              "      <td>0.018959</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.005109</td>\n",
              "      <td>0.019805</td>\n",
              "      <td>0.050853</td>\n",
              "      <td>0.002715</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>029858_02</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.002271</td>\n",
              "      <td>0.050844</td>\n",
              "      <td>0.019758</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>029858_03</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.016737</td>\n",
              "      <td>0.027537</td>\n",
              "      <td>0.000630</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000065</td>\n",
              "      <td>0.000536</td>\n",
              "      <td>0.000149</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>029858_05</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001633</td>\n",
              "      <td>0.511807</td>\n",
              "      <td>0.482639</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.014400</td>\n",
              "      <td>0.638537</td>\n",
              "      <td>2.119954</td>\n",
              "      <td>3.255226</td>\n",
              "      <td>1.559427</td>\n",
              "      <td>0.248067</td>\n",
              "      <td>0.006010</td>\n",
              "      <td>0.000047</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>029858_07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.786591</td>\n",
              "      <td>0.95644</td>\n",
              "      <td>1.048126</td>\n",
              "      <td>1.720377</td>\n",
              "      <td>2.71852</td>\n",
              "      <td>2.714576</td>\n",
              "      <td>1.802652</td>\n",
              "      <td>0.825361</td>\n",
              "      <td>0.680822</td>\n",
              "      <td>1.105280</td>\n",
              "      <td>2.600340</td>\n",
              "      <td>3.956012</td>\n",
              "      <td>4.343275</td>\n",
              "      <td>4.530775</td>\n",
              "      <td>5.214670</td>\n",
              "      <td>6.720001</td>\n",
              "      <td>9.166518</td>\n",
              "      <td>12.553081</td>\n",
              "      <td>13.133927</td>\n",
              "      <td>13.15739</td>\n",
              "      <td>10.72173</td>\n",
              "      <td>8.43653</td>\n",
              "      <td>9.794526</td>\n",
              "      <td>10.642732</td>\n",
              "      <td>6.829975</td>\n",
              "      <td>2.44</td>\n",
              "      <td>0.955724</td>\n",
              "      <td>0.367377</td>\n",
              "      <td>0.293085</td>\n",
              "      <td>0.469685</td>\n",
              "      <td>0.99838</td>\n",
              "      <td>1.506446</td>\n",
              "      <td>1.331174</td>\n",
              "      <td>0.934452</td>\n",
              "      <td>1.041506</td>\n",
              "      <td>1.967493</td>\n",
              "      <td>3.363364</td>\n",
              "      <td>3.624056</td>\n",
              "      <td>2.504621</td>\n",
              "      <td>1.668317</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2411</th>\n",
              "      <td>031287_08</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2412</th>\n",
              "      <td>031288_01</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.004317</td>\n",
              "      <td>0.01708</td>\n",
              "      <td>0.031579</td>\n",
              "      <td>0.014207</td>\n",
              "      <td>0.001473</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000556</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2413</th>\n",
              "      <td>031288_02</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005137</td>\n",
              "      <td>0.025035</td>\n",
              "      <td>0.001198</td>\n",
              "      <td>0.002221</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2414</th>\n",
              "      <td>031288_08</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2415</th>\n",
              "      <td>031288_11</td>\n",
              "      <td>0.490991</td>\n",
              "      <td>0.504081</td>\n",
              "      <td>0.638930</td>\n",
              "      <td>0.538315</td>\n",
              "      <td>0.450528</td>\n",
              "      <td>0.415894</td>\n",
              "      <td>0.237455</td>\n",
              "      <td>0.065052</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000337</td>\n",
              "      <td>0.013114</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2416 rows × 1601 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             id      px_1      px_2  ...   px_1598   px_1599   px_1600\n",
              "0     029858_01  0.055712  0.054163  ...  0.000000  0.000000  0.000000\n",
              "1     029858_02  0.000000  0.000000  ...  0.000000  0.000000  0.000000\n",
              "2     029858_03  0.000000  0.000000  ...  0.000000  0.000000  0.000000\n",
              "3     029858_05  0.000000  0.000000  ...  0.000000  0.000000  0.000000\n",
              "4     029858_07  0.000000  0.000000  ...  3.624056  2.504621  1.668317\n",
              "...         ...       ...       ...  ...       ...       ...       ...\n",
              "2411  031287_08  0.000000  0.000000  ...  0.000000  0.000000  0.000000\n",
              "2412  031288_01  0.000000  0.000000  ...  0.000000  0.000000  0.000000\n",
              "2413  031288_02  0.000000  0.000000  ...  0.025035  0.001198  0.002221\n",
              "2414  031288_08  0.000000  0.000000  ...  0.000000  0.000000  0.000000\n",
              "2415  031288_11  0.490991  0.504081  ...  0.000000  0.000000  0.000000\n",
              "\n",
              "[2416 rows x 1601 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    }
  ]
}